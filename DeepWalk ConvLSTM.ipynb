{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "import solver\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from cell import ConvLSTMCell\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/largefeatures/large200.pickle\")\n",
    "# # Drop rows with NA\n",
    "# rowsBefore = df.shape[0]\n",
    "# df = df.dropna()\n",
    "# print(\"Dropped %d rows due to None values\" % (rowsBefore - df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDeepWalkInstance(path):\n",
    "    file = open(path, \"r\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    nodeCount = None\n",
    "    shape = None\n",
    "    \n",
    "    instance = None\n",
    "    \n",
    "    for line in file:\n",
    "        if i == 0:\n",
    "            split = line.split(\" \")\n",
    "            nodeCount = int(split[0])\n",
    "            length = int(split[1])\n",
    "            \n",
    "            instance = np.zeros(shape=(nodeCount, length))\n",
    "        else:\n",
    "            split = line.split(\" \")\n",
    "            \n",
    "            node = split[0]\n",
    "            encoding = np.array(list(map(float, split[1:])))\n",
    "            \n",
    "            instance[i - 1] = encoding\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    return instance\n",
    "\n",
    "def loadDeepWalkInstances(path):\n",
    "    instances = []\n",
    "    names = []\n",
    "    for file in glob.glob(path + \"*.deep\"):\n",
    "        try:\n",
    "            instance = loadDeepWalkInstance(file)\n",
    "            split = os.path.splitext(os.path.splitext(os.path.basename(file))[0])\n",
    "            modifier = split[1][-1:]\n",
    "            name = split[0] + modifier\n",
    "\n",
    "            instances.append(instance)\n",
    "            names.append(name)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return instances, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances, names = loadDeepWalkInstances(\"../data/largedeeptest2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIZE = 300\n",
    "INSTANCE_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in DeepWalk data\n",
    "dwInstances = pd.DataFrame(columns=[\"name\", \"deepWalk\", \"sequenceLength\"])\n",
    "reshapedInstances = []\n",
    "for index, name in enumerate(names):\n",
    "    instance = instances[index]\n",
    "    instance = instance.reshape(-1)\n",
    "    \n",
    "    size = instance.shape[0]\n",
    "    \n",
    "    if name == \"pr2392\":\n",
    "        continue\n",
    "    \n",
    "    if size >= INSTANCE_SIZE * MAX_SIZE:\n",
    "        print(instances[index].shape)\n",
    "        print(\"Instance %s is too large\" % (name))\n",
    "    \n",
    "    zeroed = np.zeros((INSTANCE_SIZE * MAX_SIZE))\n",
    "    zeroed[0: size] = instance\n",
    "    \n",
    "#     instance = scale(zeroed.astype('float64')).reshape(MAX_SIZE, MAX_SIZE)\n",
    "    instance = zeroed.astype('float64').reshape(MAX_SIZE, INSTANCE_SIZE)\n",
    "        \n",
    "#     reshapedInstances.append(scale(zeroed.astype('float64')).reshape(MAX_SIZE, MAX_SIZE))\n",
    "    \n",
    "#     reshapedInstances.append(instance)\n",
    "#     instance = scale(instance.astype('float64'),axis=1)\n",
    "    dwInstances = dwInstances.append(pd.DataFrame([[name, instance, size]], columns=[\"name\", \"deepWalk\", \"sequenceLength\"]))\n",
    "    \n",
    "dwInstances = dwInstances.reset_index().drop(\"index\", axis=1)\n",
    "df = pd.merge(df, dwInstances, on=\"name\")\n",
    "df = df.drop(\"costs\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2818"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"../data/largefeatures/deep128large200noscaleReshaped.pickle\")\n",
    "# df = pd.read_pickle(\"../data/largefeatures/deeplarge200.pickle\")\n",
    "df = pd.read_pickle(\"../data/largefeatures/deep128large200noscaleReshaped.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"metadata.isAsymmetric\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)\n",
    "# minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'heuristics.antColonyCosts': 82,\n",
       "         'heuristics.geneticCosts': 1,\n",
       "         'heuristics.graspCosts': 1298,\n",
       "         'heuristics.simulatedAnnealingCosts': 10,\n",
       "         'heuristics.tabuCosts': 333})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(minCostIndices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array(df[\"deepWalk\"].tolist())\n",
    "sequenceLengths = np.array(df[\"sequenceLength\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "costValues = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].values\n",
    "indexRankings = costValues.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 4, 1, 3],\n",
       "       [0, 2, 4, 1, 3],\n",
       "       [0, 2, 4, 1, 3],\n",
       "       ..., \n",
       "       [2, 1, 0, 4, 3],\n",
       "       [0, 4, 2, 3, 1],\n",
       "       [1, 3, 2, 0, 4]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexRankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intLabels = LabelEncoder().fit_transform(minCostIndices).reshape(-1, 1)\n",
    "# # 5 values for 5 different heuristics\n",
    "# # Drop grasp from analysis\n",
    "# outputs = OneHotEncoder(sparse=False, n_values=5).fit_transform(intLabels)\n",
    "\n",
    "# inputs = df\n",
    "\n",
    "size = df.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "lengthsTrain = sequenceLengths[0:trainSize]\n",
    "outputsTrainUnnorm = indexRankings[0:trainSize]\n",
    "outputsTrain = normalize(outputsTrainUnnorm)\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "lengthsValid = sequenceLengths[trainSize:]\n",
    "outputsValidUnnorm = indexRankings[trainSize:]\n",
    "outputsValid = normalize(outputsValidUnnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "N1 = trainSize\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 512\n",
    "NODES3 = 256\n",
    "NODES4 = 256\n",
    "NODES5 = 128\n",
    "NODES6 = 128\n",
    "NODES7 = 64\n",
    "\n",
    "LSTM_SIZE = 150\n",
    "LSTM_LAYER_COUNT = 2\n",
    "LSTM_DROPOUT_PROB = 0.7\n",
    "\n",
    "ALPHA = 0.001\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "STD = 0.1\n",
    "\n",
    "LEARNING_RATE = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "inputFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsTrain.astype(np.float32), \"length\": lengthsTrain.astype(np.int32)}, y=outputsTrainUnnorm.astype(np.float32),\n",
    "#     batch_size=BATCH_SIZE, num_epochs=EPOCHS, shuffle=True)\n",
    "    num_epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsTrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsTrain[0][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def network(xDict, mode):\n",
    "    x = xDict[\"input\"]\n",
    "        \n",
    "    length = xDict[\"length\"]\n",
    "    \n",
    "    print(x.shape)\n",
    "        \n",
    "    # Batch size, timeseries, shape, channels\n",
    "    x = tf.reshape(x, shape=[-1, MAX_SIZE, INSTANCE_SIZE, 1])\n",
    "    \n",
    "    print(x.shape)\n",
    "\n",
    "#     if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#         x = tf.nn.dropout(x, LSTM_DROPOUT_PROB)\n",
    " \n",
    "#     with tf.variable_scope('lstm1'):\n",
    "#         initialCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "#     initialCell = tf.contrib.rnn.Conv2DLSTMCell(input_shape=(300, 300), output_channels=LSTM_SIZE, kernel_shape=(5,5))\n",
    "    cell = ConvLSTMCell([INSTANCE_SIZE], 1, [5])\n",
    "    lstmOutput, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32, sequence_length=length)\n",
    "    \n",
    "#     lstmOutput, state = tf.nn.dynamic_rnn(initialCell, x, dtype=tf.float32, sequence_length=length)\n",
    "        \n",
    "#         lstmOutput, _ = initialCell(x, dtype=tf.float32, sequence_length=length)\n",
    "#         lstmOutput, _ = initialCell(x, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "#     if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#         initialCell = tf.contrib.rnn.DropoutWrapper(initialCell, input_keep_prob=LSTM_DROPOUT_PROB, output_keep_prob=LSTM_DROPOUT_PROB, state_keep_prob=LSTM_DROPOUT_PROB, variational_recurrent=True, input_size=x.shape[2], dtype=tf.float64)\n",
    "    \n",
    "#     with tf.variable_scope('lstm2'):\n",
    "#         secondCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "        \n",
    "#         lstmOutput, _ = secondCell(lstmOutput, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "            \n",
    "#     with tf.variable_scope('lstm3'):\n",
    "#         thirdCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "        \n",
    "#         lstmOutput, _ = thirdCell(lstmOutput, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "            \n",
    "#     with tf.variable_scope('lstm4'):\n",
    "#         fourthCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "        \n",
    "#         lstmOutput, _ = fourthCell(lstmOutput, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "            \n",
    "#     with tf.variable_scope('lstm5'):\n",
    "#         fifthCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "        \n",
    "#         lstmOutput, _ = fifthCell(lstmOutput, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "            \n",
    "#     with tf.variable_scope('lstm6'):\n",
    "#         sixthCell = tf.contrib.rnn.LSTMBlockFusedCell(LSTM_SIZE)\n",
    "        \n",
    "#         lstmOutput, _ = sixthCell(lstmOutput, dtype=tf.float32)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             lstmOutput = tf.nn.dropout(lstmOutput, LSTM_DROPOUT_PROB)\n",
    "        \n",
    "#         if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "#             secondCell = tf.contrib.rnn.DropoutWrapper(secondCell, input_keep_prob=LSTM_DROPOUT_PROB, output_keep_prob=LSTM_DROPOUT_PROB, state_keep_prob=LSTM_DROPOUT_PROB, variational_recurrent=True, input_size=initialCell.output_size, dtype=tf.float64)\n",
    "        \n",
    "#     stackedLstm = tf.contrib.rnn.MultiRNNCell([initialCell, secondCell])\n",
    "        \n",
    "#     lstmOutput, _ = tf.nn.dynamic_rnn(stackedLstm, x, dtype=tf.float64, sequence_length=length)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(lstmOutput)\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=ALPHA)\n",
    "    \n",
    "    # Hidden fully connected layer\n",
    "#     layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "#     layer2 = tf.layers.dense(layer1, NODES2, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "#     layer3 = tf.layers.dense(layer2, NODES3, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    layer1 = tf.layers.dense(flatten, NODES1, activation=tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, NODES2, activation=tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, NODES3, activation=tf.nn.relu)\n",
    "    layer4 = tf.layers.dense(layer3, NODES4, activation=tf.nn.relu)\n",
    "    layer5 = tf.layers.dense(layer4, NODES5, activation=tf.nn.relu)\n",
    "    layer6 = tf.layers.dense(layer5, NODES6, activation=tf.nn.relu)\n",
    "    layer7 = tf.layers.dense(layer6, NODES7, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    outLayer = tf.layers.dense(layer7, LABEL_COUNT)\n",
    "    return outLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler Divergence, as per https://stackoverflow.com/a/43298483\n",
    "def klDivergence(p, q):\n",
    "    pClipped = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "    qClipped = tf.clip_by_value(q, 1e-10, 1.0)\n",
    "    return tf.reduce_sum(pClipped * tf.log(pClipped/qClipped))\n",
    "\n",
    "# Loss function based off of Jensen-Shannon Divergence\n",
    "def loss(label, prediction):\n",
    "    mean = 0.5 * (label + prediction)\n",
    "    return 0.5 * klDivergence(label, mean) + 0.5 * klDivergence(prediction, mean)\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "def listNetLoss(label, prediction):\n",
    "    softMaxLabel = tf.nn.softmax(label)\n",
    "    softMaxPrediction = tf.nn.softmax(prediction)\n",
    "    return -tf.reduce_mean(softMaxLabel * tf.log(softMaxPrediction))\n",
    "\n",
    "def listMLE(label, prediction):\n",
    "    sortedPrediction = tf.gather(prediction, tf.nn.top_k(label, k=5).indices)\n",
    "    final = tf.log(tf.reduce_sum(tf.exp(sortedPrediction)))\n",
    "    return tf.reduce_sum(final - sortedPrediction)\n",
    "\n",
    "def listMLE2Loss(labels, predictions, length, length64):\n",
    "    i = tf.constant(0, dtype=tf.int32)\n",
    "    innerSum = tf.constant(0, dtype=tf.float32)\n",
    "    \n",
    "    def loop(label, prediction, i, innerSum):\n",
    "        return tf.add(i, 1), tf.add(innerSum, listMLE2(label, prediction))\n",
    "    \n",
    "    cond = lambda i, _: tf.less(i, length)\n",
    "    operation = lambda i, innerSum: loop(labels[i], predictions[i], i, innerSum)\n",
    "    result = tf.while_loop(cond, operation, [i, innerSum])\n",
    "\n",
    "    return result[1]/length64\n",
    "#     return tf.constant(1.0, dtype=tf.float64) * labels + predictions\n",
    "\n",
    "def listMLE2(label, prediction):\n",
    "    # Length of vectors\n",
    "    k = tf.constant(LABEL_COUNT, dtype=tf.int32)\n",
    "    \n",
    "    sortedPrediction = tf.gather(prediction, tf.nn.top_k(label, k=k).indices)\n",
    "    \n",
    "    j = tf.constant(0, dtype=tf.int32)\n",
    "    innerSum = tf.constant(0, dtype=tf.float32)\n",
    "    cond = lambda j, _: tf.less(j, k)\n",
    "    operation = lambda j, innerSum: listMLE2Loop(sortedPrediction, j, k, innerSum)\n",
    "    result = tf.while_loop(cond, operation, [j, innerSum])\n",
    "    \n",
    "    print(result[1].shape)\n",
    "    \n",
    "    return -result[1]\n",
    "    \n",
    "def listMLE2Loop(sortedPrediction, j, k, innerSum):\n",
    "    return tf.add(j, 1), tf.add(innerSum, listMLE2Inner(sortedPrediction, j, k))\n",
    "\n",
    "def listMLE2Inner(sortedPrediction, j, k):\n",
    "    numerator = tf.exp(tf.gather(sortedPrediction, j))\n",
    "    denominator = tf.reduce_sum(tf.exp(sortedPrediction[j:k]))\n",
    "    \n",
    "    return tf.log(numerator/denominator)\n",
    "\n",
    "# Builds an integer ranking out of a 1-D tensor\n",
    "def convertPredToRank(prediction):\n",
    "    return tf.cast(tf.nn.top_k(prediction, k=5).indices, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric using Normalized Discounted Cumulative Gain, as per https://github.com/shiba24/learning2rank/\n",
    "def ndcg(labels, predictions, k=5):\n",
    "    topK = tf.nn.top_k(labels, k=5)\n",
    "    sortedValues = topK.values\n",
    "    sortedIndices = topK.indices\n",
    "#         print(labelSorted)\n",
    "#         labelSorted = sorted(label, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(k):\n",
    "#             ideal_dcg += (2 ** labelSorted[:i] - 1.) / log2(tf.cast(i + 2, tf.float64))\n",
    "        ideal_dcg += (tf.cast(sortedValues[i] + 1, tf.float32)) / log2(tf.cast(i + 2, tf.float32))\n",
    "    dcg = 0\n",
    "#         argsort_indices = np.argsort(predictions)[::-1]\n",
    "#         argsort_indices = tf.nn.top_k(predictions, k=5).indices\n",
    "#         print(argsort_indices)\n",
    "    for i in range(k):\n",
    "        dcg += (tf.gather(predictions, sortedIndices[i]) + 1) / log2(tf.cast(i + 2, tf.float32))\n",
    "#         dcg += (predictions[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    return dcg / ideal_dcg\n",
    "\n",
    "def spearmanCorrelation(label, prediction):\n",
    "    length = tf.cast(tf.shape(prediction)[0], tf.float32)\n",
    "    sumVal = tf.reduce_sum(tf.square(tf.subtract(prediction, label)))\n",
    "    return 1 - 6 * sumVal / (length ** 3 - length)\n",
    "\n",
    "# Bound Spearman coeff. between 0 and 1\n",
    "def boundedSpearman(label, prediction):\n",
    "    return (spearmanCorrelation(label, prediction) + 1.)/2\n",
    "\n",
    "def top1Match(label, prediction):\n",
    "    return tf.cast(tf.equal(label[0], prediction[0]), tf.float32)\n",
    "\n",
    "def top2Match(label, prediction):\n",
    "    sameFirstOrSecond = tf.logical_or(tf.equal(label[0], prediction[0]), tf.equal(label[1], prediction[1]))\n",
    "    sameFirstAndSecond = tf.logical_or(tf.equal(label[1], prediction[0]), tf.equal(label[0], prediction[1]))\n",
    "    return tf.cast(tf.logical_or(sameFirstOrSecond, sameFirstAndSecond), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model function (following TF Estimator Template)\n",
    "# def modelFunc(features, labels, mode):\n",
    "#     # Build the neural network\n",
    "#     logits = network(features)\n",
    "    \n",
    "# #     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "#     # Predictions\n",
    "#     # TODO: Possibly need to change\n",
    "#     pred_classes = logits\n",
    "# #     pred_classes = tf.argmax(logits, axis=1)\n",
    "# #     pred_probas = tf.nn.softmax(logits)\n",
    "#     pred_probas = tf.nn.sigmoid(logits)\n",
    "    \n",
    "#     # If prediction mode, early return\n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#         return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "#     print(logits.shape)\n",
    "# #     print(resizedLogits.shape)\n",
    "#     print(labels.shape)\n",
    "#     print(pred_classes.shape)\n",
    "        \n",
    "#     # Define loss and optimizer\n",
    "# #     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "# #         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "#     train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "#     # Evaluate the accuracy of the model\n",
    "# #     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "#     # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "#     # the different ops for training, evaluating, ...\n",
    "#     estim_specs = tf.estimator.EstimatorSpec(\n",
    "#       mode=mode,\n",
    "#       predictions=pred_classes,\n",
    "#       loss=loss_op,\n",
    "#       train_op=train_op,\n",
    "#       eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "#     return estim_specs\n",
    "\n",
    "# Define the model function (following TF Estimator Template)\n",
    "def modelFunc(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    logits = network(features, mode)\n",
    "    \n",
    "#     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "    # Predictions\n",
    "    # TODO: Possibly need to change\n",
    "#     pred_classes = logits\n",
    "    pred_classes = tf.map_fn(convertPredToRank, logits)\n",
    "#     pred_classes = tf.argmax(logits, axis=1)\n",
    "#     pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "#     loss_op = tf.reduce_mean(loss(labels, logits))\n",
    "    loss_op = tf.reduce_mean(listNetLoss(labels, logits))\n",
    "#     loss_map = tf.map_fn(lambda x: listMLE2(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "#     print(labels.get_shape()[0])\n",
    "#     labels_length = tf.shape(labels)[0]\n",
    "#     loss_op = tf.reduce_mean(listMLE2Loss(labels, logits, labels_length, tf.cast(labels_length, dtype=tf.float32)))\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "    optimizer = tf.contrib.opt.NadamOptimizer(learning_rate=LEARNING_RATE)\n",
    "#     optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "#     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    ndcg_map = tf.map_fn(lambda x: ndcg(x[0], x[1]), (labels, pred_classes), dtype=tf.float32)\n",
    "    ndcg_op = tf.metrics.mean(ndcg_map)\n",
    "    top1_map = tf.map_fn(lambda x: top1Match(x[0], x[1]), (labels, pred_classes), dtype=tf.float32)\n",
    "    top1_op = tf.metrics.mean(top1_map)\n",
    "    top2_map = tf.map_fn(lambda x: top2Match(x[0], x[1]), (labels, pred_classes), dtype=tf.float32)\n",
    "    top2_op = tf.metrics.mean(top2_map)\n",
    "    spearman_map = tf.map_fn(lambda x: boundedSpearman(x[0], x[1]), (labels, pred_classes), dtype=tf.float32)\n",
    "    acc_op = tf.metrics.mean(spearman_map)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op, 'ndcg': ndcg_op, 'top1Classification': top1_op, 'top2Classification': top2_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprdl9jn_u\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3d193a0748>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 10000, '_save_checkpoints_secs': 600, '_log_step_count_steps': 10000, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.8\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmprdl9jn_u'}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "model = tf.estimator.Estimator(modelFunc, config=tf.contrib.learn.RunConfig(session_config=config, save_summary_steps=10000, log_step_count_steps=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainResults = []\n",
    "validResults = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300, 128)\n",
      "(?, 300, 128, 1)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.324119, step = 1\n",
      "INFO:tensorflow:loss = 0.263792, step = 101 (207.083 sec)\n",
      "INFO:tensorflow:loss = 0.253956, step = 201 (199.799 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 298 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.243236, step = 301 (199.477 sec)\n",
      "INFO:tensorflow:loss = 0.235455, step = 401 (199.236 sec)\n",
      "INFO:tensorflow:loss = 0.226293, step = 501 (199.622 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 597 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.21949, step = 601 (203.229 sec)\n",
      "INFO:tensorflow:loss = 0.216227, step = 701 (202.942 sec)\n",
      "INFO:tensorflow:loss = 0.21628, step = 801 (197.068 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 899 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.208889, step = 901 (197.352 sec)\n",
      "INFO:tensorflow:loss = 0.206022, step = 1001 (201.976 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1011 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.205173.\n",
      "Evaluating Train\n",
      "(?, 300, 128)\n",
      "(?, 300, 128, 1)\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-06-13:34:20\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprdl9jn_u/model.ckpt-1011\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-06-13:34:36\n",
      "INFO:tensorflow:Saving dict for global step 1011: accuracy = 0.493194, global_step = 1011, loss = 0.206299, ndcg = 0.859513, top1Classification = 0.304718, top2Classification = 0.709203\n",
      "Evaluating Valid\n",
      "(?, 300, 128)\n",
      "(?, 300, 128, 1)\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-06-13:34:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprdl9jn_u/model.ckpt-1011\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-06-13:34:45\n",
      "INFO:tensorflow:Saving dict for global step 1011: accuracy = 0.479118, global_step = 1011, loss = 0.380122, ndcg = 0.857503, top1Classification = 0.141531, top2Classification = 0.675174\n",
      "(?, 300, 128)\n",
      "(?, 300, 128, 1)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmprdl9jn_u/model.ckpt-1011\n",
      "INFO:tensorflow:Saving checkpoints for 1012 into /tmp/tmprdl9jn_u/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.206167, step = 1012\n",
      "INFO:tensorflow:loss = 0.21176, step = 1112 (213.247 sec)\n",
      "INFO:tensorflow:loss = 0.206685, step = 1212 (210.150 sec)\n"
     ]
    }
   ],
   "source": [
    "trainFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsTrain.astype(np.float32), \"length\": lengthsTrain}, y=outputsTrainUnnorm.astype(np.float32),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid.astype(np.float32), \"length\": lengthsValid}, y=outputsValidUnnorm.astype(np.float32),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for i in range(0, 100):\n",
    "    model.train(inputFunc, steps=20000)\n",
    "    \n",
    "    print(\"Evaluating Train\")\n",
    "    accuracy = model.evaluate(trainFunc)\n",
    "    trainResults.append(accuracy)\n",
    "    \n",
    "    print(\"Evaluating Valid\")\n",
    "    accuracy = model.evaluate(validFunc)\n",
    "    validResults.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.49319422,\n",
       "  'global_step': 1011,\n",
       "  'loss': 0.20629933,\n",
       "  'ndcg': 0.85951304,\n",
       "  'top1Classification': 0.30471772,\n",
       "  'top2Classification': 0.70920342},\n",
       " {'accuracy': 0.44083521,\n",
       "  'global_step': 2022,\n",
       "  'loss': 0.20306441,\n",
       "  'ndcg': 0.84175223,\n",
       "  'top1Classification': 0.30317092,\n",
       "  'top2Classification': 0.69914925},\n",
       " {'accuracy': 0.43638828,\n",
       "  'global_step': 3033,\n",
       "  'loss': 0.20072538,\n",
       "  'ndcg': 0.84082532,\n",
       "  'top1Classification': 0.30394432,\n",
       "  'top2Classification': 0.70533645},\n",
       " {'accuracy': 0.43700695,\n",
       "  'global_step': 4044,\n",
       "  'loss': 0.20037386,\n",
       "  'ndcg': 0.84111071,\n",
       "  'top1Classification': 0.30394432,\n",
       "  'top2Classification': 0.70533645},\n",
       " {'accuracy': 0.43669766,\n",
       "  'global_step': 5055,\n",
       "  'loss': 0.20063591,\n",
       "  'ndcg': 0.84101373,\n",
       "  'top1Classification': 0.30471772,\n",
       "  'top2Classification': 0.70533645},\n",
       " {'accuracy': 0.43681359,\n",
       "  'global_step': 6066,\n",
       "  'loss': 0.20008318,\n",
       "  'ndcg': 0.84107918,\n",
       "  'top1Classification': 0.30394432,\n",
       "  'top2Classification': 0.70533645},\n",
       " {'accuracy': 0.43607888,\n",
       "  'global_step': 7077,\n",
       "  'loss': 0.20045839,\n",
       "  'ndcg': 0.84091556,\n",
       "  'top1Classification': 0.30394432,\n",
       "  'top2Classification': 0.70533645},\n",
       " {'accuracy': 0.43681359,\n",
       "  'global_step': 8088,\n",
       "  'loss': 0.20017147,\n",
       "  'ndcg': 0.84107918,\n",
       "  'top1Classification': 0.30394432,\n",
       "  'top2Classification': 0.70533645}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.47911829,\n",
       "  'global_step': 1011,\n",
       "  'loss': 0.38012153,\n",
       "  'ndcg': 0.85750318,\n",
       "  'top1Classification': 0.14153132,\n",
       "  'top2Classification': 0.675174},\n",
       " {'accuracy': 0.46972162,\n",
       "  'global_step': 2022,\n",
       "  'loss': 0.39480475,\n",
       "  'ndcg': 0.85565573,\n",
       "  'top1Classification': 0.13921113,\n",
       "  'top2Classification': 0.63109046},\n",
       " {'accuracy': 0.46600926,\n",
       "  'global_step': 3033,\n",
       "  'loss': 0.39247605,\n",
       "  'ndcg': 0.8527562,\n",
       "  'top1Classification': 0.1438515,\n",
       "  'top2Classification': 0.66589326},\n",
       " {'accuracy': 0.4575406,\n",
       "  'global_step': 4044,\n",
       "  'loss': 0.39723518,\n",
       "  'ndcg': 0.85106581,\n",
       "  'top1Classification': 0.13689095,\n",
       "  'top2Classification': 0.6403712},\n",
       " {'accuracy': 0.47169375,\n",
       "  'global_step': 5055,\n",
       "  'loss': 0.39897355,\n",
       "  'ndcg': 0.85377109,\n",
       "  'top1Classification': 0.13689095,\n",
       "  'top2Classification': 0.68213457},\n",
       " {'accuracy': 0.461833,\n",
       "  'global_step': 6066,\n",
       "  'loss': 0.39418501,\n",
       "  'ndcg': 0.85060596,\n",
       "  'top1Classification': 0.12761021,\n",
       "  'top2Classification': 0.66357309},\n",
       " {'accuracy': 0.47575408,\n",
       "  'global_step': 7077,\n",
       "  'loss': 0.39505532,\n",
       "  'ndcg': 0.85417473,\n",
       "  'top1Classification': 0.11832947,\n",
       "  'top2Classification': 0.66357309},\n",
       " {'accuracy': 0.48457077,\n",
       "  'global_step': 8088,\n",
       "  'loss': 0.39398947,\n",
       "  'ndcg': 0.85920292,\n",
       "  'top1Classification': 0.13921113,\n",
       "  'top2Classification': 0.675174}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-01-13:09:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpg3jck0j2/model.ckpt-101100\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-01-13:09:27\n",
      "INFO:tensorflow:Saving dict for global step 101100: accuracy = 0.534803, global_step = 101100, loss = 0.317587, ndcg = 0.871996, top1Classification = 0.577726, top2Classification = 0.958237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.53480279,\n",
       " 'global_step': 101100,\n",
       " 'loss': 0.31758687,\n",
       " 'ndcg': 0.87199575,\n",
       " 'top1Classification': 0.57772624,\n",
       " 'top2Classification': 0.95823663}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid.astype(np.float32), \"length\": lengthsValid}, y=outputsValidUnnorm.astype(np.float32),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(validFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpg3jck0j2/model.ckpt-101100\n"
     ]
    }
   ],
   "source": [
    "predictions = list(model.predict(validFunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[ 2.  1.  3.  4.  0.]': 431})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(list(map(str, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[0 1 2 3 4]': 4,\n",
       "         '[0 1 2 4 3]': 3,\n",
       "         '[0 1 4 2 3]': 1,\n",
       "         '[0 2 1 3 4]': 1,\n",
       "         '[0 2 1 4 3]': 100,\n",
       "         '[0 2 3 4 1]': 4,\n",
       "         '[0 2 4 1 3]': 196,\n",
       "         '[0 2 4 3 1]': 22,\n",
       "         '[0 4 2 1 3]': 1,\n",
       "         '[0 4 2 3 1]': 1,\n",
       "         '[1 2 0 4 3]': 2,\n",
       "         '[1 2 3 0 4]': 2,\n",
       "         '[1 2 3 4 0]': 1,\n",
       "         '[1 2 4 0 3]': 4,\n",
       "         '[1 3 2 0 4]': 1,\n",
       "         '[2 0 1 3 4]': 5,\n",
       "         '[2 0 1 4 3]': 198,\n",
       "         '[2 0 3 1 4]': 3,\n",
       "         '[2 0 3 4 1]': 12,\n",
       "         '[2 0 4 1 3]': 198,\n",
       "         '[2 0 4 3 1]': 195,\n",
       "         '[2 1 0 4 3]': 4,\n",
       "         '[2 1 3 4 0]': 6,\n",
       "         '[2 1 4 0 3]': 20,\n",
       "         '[2 1 4 3 0]': 5,\n",
       "         '[2 3 0 4 1]': 2,\n",
       "         '[2 3 1 4 0]': 4,\n",
       "         '[2 3 4 0 1]': 5,\n",
       "         '[2 3 4 1 0]': 9,\n",
       "         '[2 4 0 1 3]': 195,\n",
       "         '[2 4 0 3 1]': 177,\n",
       "         '[2 4 1 0 3]': 68,\n",
       "         '[2 4 1 3 0]': 101,\n",
       "         '[2 4 3 0 1]': 42,\n",
       "         '[2 4 3 1 0]': 49,\n",
       "         '[3 4 1 2 0]': 1,\n",
       "         '[4 0 1 2 3]': 1,\n",
       "         '[4 0 2 1 3]': 10,\n",
       "         '[4 0 2 3 1]': 3,\n",
       "         '[4 1 2 0 3]': 2,\n",
       "         '[4 1 2 3 0]': 5,\n",
       "         '[4 2 0 1 3]': 34,\n",
       "         '[4 2 0 3 1]': 6,\n",
       "         '[4 2 1 0 3]': 8,\n",
       "         '[4 2 1 3 0]': 3,\n",
       "         '[4 2 3 1 0]': 8,\n",
       "         '[4 3 1 2 0]': 1,\n",
       "         '[4 3 2 0 1]': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(list(map(str, indexRankings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.43051046,\n",
       " 'global_step': 404100,\n",
       " 'loss': 0.21140459,\n",
       " 'ndcg': 0.84055918,\n",
       " 'top1Classification': 0.31090486,\n",
       " 'top2Classification': 0.71616393}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainResults[len(trainResults) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.53317863,\n",
       " 'global_step': 404100,\n",
       " 'loss': 0.4090046,\n",
       " 'ndcg': 0.8764475,\n",
       " 'top1Classification': 0.15777262,\n",
       " 'top2Classification': 0.77030164}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validResults[len(validResults) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
