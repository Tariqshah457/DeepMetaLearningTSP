{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import glob, os, json\n",
    "import solver\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/features/optGenAntAsymAnalysis.pickle\")\n",
    "# # Drop rows with NA\n",
    "# rowsBefore = df.shape[0]\n",
    "# df = df.dropna()\n",
    "# print(\"Dropped %d rows due to None values\" % (rowsBefore - df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDeepWalkInstance(path):\n",
    "    file = open(path, \"r\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    nodeCount = None\n",
    "    shape = None\n",
    "    \n",
    "    instance = None\n",
    "    \n",
    "    for line in file:\n",
    "        if i == 0:\n",
    "            split = line.split(\" \")\n",
    "            nodeCount = int(split[0])\n",
    "            length = int(split[1])\n",
    "            \n",
    "            instance = np.zeros(shape=(nodeCount, length))\n",
    "        else:\n",
    "            split = line.split(\" \")\n",
    "            \n",
    "            node = split[0]\n",
    "            encoding = np.array(list(map(float, split[1:])))\n",
    "            \n",
    "            instance[i - 1] = encoding\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    return instance\n",
    "\n",
    "def loadDeepWalkInstances(path):\n",
    "    instances = []\n",
    "    names = []\n",
    "    for file in glob.glob(path + \"*.deep\"):\n",
    "        try:\n",
    "            instance = loadDeepWalkInstance(file)\n",
    "            name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "\n",
    "            instances.append(instance)\n",
    "            names.append(name)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return instances, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances, names = loadDeepWalkInstances(\"../data/deepwalk2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in DeepWalk data\n",
    "dwInstances = pd.DataFrame(columns=[\"name\", \"deepWalk\", \"sequenceLength\"])\n",
    "reshapedInstances = []\n",
    "for index, name in enumerate(names):\n",
    "    instance = instances[index]\n",
    "    instance = instance.reshape(-1)\n",
    "    \n",
    "    size = instance.shape[0]\n",
    "    \n",
    "    if name == \"pr2392\":\n",
    "        continue\n",
    "    \n",
    "    if size >= MAX_SIZE * MAX_SIZE:\n",
    "        print(instances[index].shape)\n",
    "        print(\"Instance %s is too large\" % (name))\n",
    "    \n",
    "    zeroed = np.zeros((MAX_SIZE * MAX_SIZE))\n",
    "    zeroed[0: size] = instance\n",
    "    \n",
    "    instance = scale(zeroed.astype('float64')).reshape(MAX_SIZE, MAX_SIZE)\n",
    "        \n",
    "#     reshapedInstances.append(scale(zeroed.astype('float64')).reshape(MAX_SIZE, MAX_SIZE))\n",
    "    \n",
    "#     reshapedInstances.append(instance)\n",
    "#     instance = scale(instance.astype('float64'),axis=1)\n",
    "    dwInstances = dwInstances.append(pd.DataFrame([[name, instance, size]], columns=[\"name\", \"deepWalk\", \"sequenceLength\"]))\n",
    "    \n",
    "dwInstances = dwInstances.reset_index().drop(\"index\", axis=1)\n",
    "df = pd.merge(df, dwInstances, on=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)\n",
    "# minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'heuristics.antColonyCosts': 454,\n",
       "         'heuristics.geneticCosts': 1,\n",
       "         'heuristics.graspCosts': 629,\n",
       "         'heuristics.simulatedAnnealingCosts': 9,\n",
       "         'heuristics.tabuCosts': 128})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(minCostIndices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.array(df[\"deepWalk\"].tolist())\n",
    "sequenceLengths = np.array(df[\"sequenceLength\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costValues = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].values\n",
    "indexRankings = costValues.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 3, 2, 0],\n",
       "       [4, 1, 3, 2, 0],\n",
       "       [4, 1, 3, 2, 0],\n",
       "       ..., \n",
       "       [2, 0, 1, 4, 3],\n",
       "       [2, 0, 1, 4, 3],\n",
       "       [0, 2, 4, 1, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexRankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intLabels = LabelEncoder().fit_transform(minCostIndices).reshape(-1, 1)\n",
    "# # 5 values for 5 different heuristics\n",
    "# # Drop grasp from analysis\n",
    "# outputs = OneHotEncoder(sparse=False, n_values=5).fit_transform(intLabels)\n",
    "\n",
    "# inputs = df\n",
    "\n",
    "size = df.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "lengthsTrain = sequenceLengths[0:trainSize]\n",
    "outputsTrainUnnorm = indexRankings[0:trainSize]\n",
    "outputsTrain = normalize(outputsTrainUnnorm)\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "lengthsValid = sequenceLengths[trainSize:]\n",
    "outputsValidUnnorm = indexRankings[trainSize:]\n",
    "outputsValid = normalize(outputsValidUnnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = trainSize\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "NODES3 = 64\n",
    "\n",
    "LSTM_SIZE = 100\n",
    "LSTM_LAYER_COUNT = 2\n",
    "LSTM_DROPOUT_PROB = 0.5\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "STD = 0.1\n",
    "\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "inputFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsTrain, \"length\": lengthsTrain}, y=outputsTrainUnnorm.astype(float),\n",
    "    batch_size=BATCH_SIZE, num_epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73029674,  0.18257419,  0.54772256,  0.36514837,  0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def network(xDict):\n",
    "    x = xDict[\"input\"]\n",
    "    \n",
    "    length = xDict[\"length\"]\n",
    "    \n",
    "#     input_layer = tf.reshape(x, shape=[-1, MAX_SIZE, MAX_SIZE, 1])\n",
    "\n",
    "    initialCell = tf.contrib.rnn.BasicLSTMCell(LSTM_SIZE)\n",
    "    initialCell = tf.contrib.rnn.DropoutWrapper(initialCell, input_keep_prob=LSTM_DROPOUT_PROB, variational_recurrent=True, input_size=x.shape[2], dtype=tf.float64)\n",
    "    \n",
    "    secondCell = tf.contrib.rnn.BasicLSTMCell(LSTM_SIZE)\n",
    "    secondCell = tf.contrib.rnn.DropoutWrapper(secondCell, input_keep_prob=LSTM_DROPOUT_PROB, variational_recurrent=True, input_size=initialCell.output_size, dtype=tf.float64)\n",
    "        \n",
    "    stackedLstm = tf.contrib.rnn.MultiRNNCell([initialCell, secondCell])\n",
    "\n",
    "    lstmOutput, _ = tf.nn.dynamic_rnn(stackedLstm, x, dtype=tf.float64, sequence_length=length)\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(lstmOutput)\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=ALPHA)\n",
    "    \n",
    "    # Hidden fully connected layer\n",
    "    layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, NODES2, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, NODES3, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    outLayer = tf.layers.dense(layer3, LABEL_COUNT)\n",
    "    return outLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kullback-Leibler Divergence, as per https://stackoverflow.com/a/43298483\n",
    "def klDivergence(p, q):\n",
    "    pClipped = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "    qClipped = tf.clip_by_value(q, 1e-10, 1.0)\n",
    "    return tf.reduce_sum(pClipped * tf.log(pClipped/qClipped))\n",
    "\n",
    "# Loss function based off of Jensen-Shannon Divergence\n",
    "def loss(label, prediction):\n",
    "    mean = 0.5 * (label + prediction)\n",
    "    return 0.5 * klDivergence(label, mean) + 0.5 * klDivergence(prediction, mean)\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "def listNetLoss(label, prediction):\n",
    "    softMaxLabel = tf.nn.softmax(label)\n",
    "    softMaxPrediction = tf.nn.softmax(prediction)\n",
    "    return -tf.reduce_mean(softMaxLabel * tf.log(softMaxPrediction))\n",
    "\n",
    "def listMLE(label, prediction):\n",
    "    sortedPrediction = tf.gather(prediction, tf.nn.top_k(label, k=5).indices)\n",
    "    final = tf.log(tf.reduce_sum(tf.exp(sortedPrediction)))\n",
    "    return tf.reduce_sum(final - sortedPrediction)\n",
    "\n",
    "def listMLE2Loss(labels, predictions, length, length64):\n",
    "    i = tf.constant(0, dtype=tf.int32)\n",
    "    innerSum = tf.constant(0, dtype=tf.float64)\n",
    "    \n",
    "    def loop(label, prediction, i, innerSum):\n",
    "        return tf.add(i, 1), tf.add(innerSum, listMLE2(label, prediction))\n",
    "    \n",
    "    cond = lambda i, _: tf.less(i, length)\n",
    "    operation = lambda i, innerSum: loop(labels[i], predictions[i], i, innerSum)\n",
    "    result = tf.while_loop(cond, operation, [i, innerSum])\n",
    "\n",
    "    return result[1]/length64\n",
    "#     return tf.constant(1.0, dtype=tf.float64) * labels + predictions\n",
    "\n",
    "def listMLE2(label, prediction):\n",
    "    # Length of vectors\n",
    "    k = tf.constant(LABEL_COUNT, dtype=tf.int32)\n",
    "    \n",
    "    sortedPrediction = tf.gather(prediction, tf.nn.top_k(label, k=k).indices)\n",
    "    \n",
    "    j = tf.constant(0, dtype=tf.int32)\n",
    "    innerSum = tf.constant(0, dtype=tf.float64)\n",
    "    cond = lambda j, _: tf.less(j, k)\n",
    "    operation = lambda j, innerSum: listMLE2Loop(sortedPrediction, j, k, innerSum)\n",
    "    result = tf.while_loop(cond, operation, [j, innerSum])\n",
    "    \n",
    "    print(result[1].shape)\n",
    "    \n",
    "    return -result[1]\n",
    "    \n",
    "def listMLE2Loop(sortedPrediction, j, k, innerSum):\n",
    "    return tf.add(j, 1), tf.add(innerSum, listMLE2Inner(sortedPrediction, j, k))\n",
    "\n",
    "def listMLE2Inner(sortedPrediction, j, k):\n",
    "    numerator = tf.exp(tf.gather(sortedPrediction, j))\n",
    "    denominator = tf.reduce_sum(tf.exp(sortedPrediction[j:k]))\n",
    "    \n",
    "    return tf.log(numerator/denominator)\n",
    "\n",
    "# Builds an integer ranking out of a 1-D tensor\n",
    "def convertPredToRank(prediction):\n",
    "    return tf.cast(tf.nn.top_k(prediction, k=5).indices, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "listMLE2Loss() missing 2 required positional arguments: 'length' and 'length64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-021bbcb4bfb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0marray1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0marray2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistMLE2Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistMLE2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistMLE2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: listMLE2Loss() missing 2 required positional arguments: 'length' and 'length64'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    array1 = tf.constant([[0, 1, 2, 3, 4]], dtype=tf.float64)\n",
    "    array2 = tf.constant([[0, 2, 3, 4, 1]], dtype=tf.float64)\n",
    "    print(listMLE2Loss(array1, array2).eval())\n",
    "    print(listMLE2(array1, array1).eval())\n",
    "    print(listMLE2(array1, array2).eval())\n",
    "    print(listMLE2(array2, array1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  1.  0.  3.  4.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    array1 = tf.constant([2, 1, 0, 3, 4], dtype=tf.float64)\n",
    "#     array2 = tf.constant([0, 2, 3, 4, 1], dtype=tf.float64)\n",
    "#     final = tf.log(tf.reduce_sum(tf.exp(array1)))\n",
    "#     print(final.eval())\n",
    "    print(array1[0:5].eval())\n",
    "#     print(tf.slice(array1, [2], [4]).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy metric using Normalized Discounted Cumulative Gain, as per https://github.com/shiba24/learning2rank/\n",
    "def ndcg(labels, predictions, k=5):\n",
    "    topK = tf.nn.top_k(labels, k=5)\n",
    "    sortedValues = topK.values\n",
    "    sortedIndices = topK.indices\n",
    "#         print(labelSorted)\n",
    "#         labelSorted = sorted(label, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(k):\n",
    "#             ideal_dcg += (2 ** labelSorted[:i] - 1.) / log2(tf.cast(i + 2, tf.float64))\n",
    "        ideal_dcg += (sortedValues[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    dcg = 0\n",
    "#         argsort_indices = np.argsort(predictions)[::-1]\n",
    "#         argsort_indices = tf.nn.top_k(predictions, k=5).indices\n",
    "#         print(argsort_indices)\n",
    "    for i in range(k):\n",
    "        dcg += (tf.gather(predictions, sortedIndices[i]) + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "#         dcg += (predictions[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    return dcg / ideal_dcg\n",
    "\n",
    "def spearmanCorrelation(label, prediction):\n",
    "    length = tf.cast(tf.shape(prediction)[0], tf.float64)\n",
    "    sumVal = tf.reduce_sum(tf.square(tf.subtract(prediction, label)))\n",
    "    return 1 - 6 * sumVal / (length ** 3 - length)\n",
    "\n",
    "# Bound Spearman coeff. between 0 and 1\n",
    "def boundedSpearman(label, prediction):\n",
    "    return (spearmanCorrelation(label, prediction) + 1.)/2\n",
    "\n",
    "def top1Match(label, prediction):\n",
    "    return tf.cast(tf.equal(label[0], prediction[0]), tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Define the model function (following TF Estimator Template)\n",
    "# def modelFunc(features, labels, mode):\n",
    "#     # Build the neural network\n",
    "#     logits = network(features)\n",
    "    \n",
    "# #     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "#     # Predictions\n",
    "#     # TODO: Possibly need to change\n",
    "#     pred_classes = logits\n",
    "# #     pred_classes = tf.argmax(logits, axis=1)\n",
    "# #     pred_probas = tf.nn.softmax(logits)\n",
    "#     pred_probas = tf.nn.sigmoid(logits)\n",
    "    \n",
    "#     # If prediction mode, early return\n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#         return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "#     print(logits.shape)\n",
    "# #     print(resizedLogits.shape)\n",
    "#     print(labels.shape)\n",
    "#     print(pred_classes.shape)\n",
    "        \n",
    "#     # Define loss and optimizer\n",
    "# #     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "# #         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "#     train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "#     # Evaluate the accuracy of the model\n",
    "# #     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "#     # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "#     # the different ops for training, evaluating, ...\n",
    "#     estim_specs = tf.estimator.EstimatorSpec(\n",
    "#       mode=mode,\n",
    "#       predictions=pred_classes,\n",
    "#       loss=loss_op,\n",
    "#       train_op=train_op,\n",
    "#       eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "#     return estim_specs\n",
    "\n",
    "# Define the model function (following TF Estimator Template)\n",
    "def modelFunc(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    logits = network(features)\n",
    "    \n",
    "#     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "    # Predictions\n",
    "    # TODO: Possibly need to change\n",
    "#     pred_classes = logits\n",
    "    pred_classes = tf.map_fn(convertPredToRank, logits)\n",
    "#     pred_classes = tf.argmax(logits, axis=1)\n",
    "#     pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "#     loss_op = tf.reduce_mean(loss(labels, logits))\n",
    "#     loss_op = tf.reduce_mean(listNetLoss(labels, logits))\n",
    "#     loss_map = tf.map_fn(lambda x: listMLE2(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "#     print(labels.get_shape()[0])\n",
    "    labels_length = tf.shape(labels)[0]\n",
    "    loss_op = tf.reduce_mean(listMLE2Loss(labels, logits, labels_length, tf.cast(labels_length, dtype=tf.float64)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "#     optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "#     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    ndcg_map = tf.map_fn(lambda x: ndcg(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "    ndcg_op = tf.metrics.mean(ndcg_map)\n",
    "    top1_map = tf.map_fn(lambda x: top1Match(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "    top1_op = tf.metrics.mean(top1_map)\n",
    "    spearman_map = tf.map_fn(lambda x: boundedSpearman(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "    acc_op = tf.metrics.mean(spearman_map)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op, 'ndcg': ndcg_op, 'top1Classification': top1_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c74dbdc18>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.5\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6'}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "model = tf.estimator.Estimator(modelFunc, config=tf.contrib.learn.RunConfig(session_config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6/model.ckpt-1000\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.18405637852, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 0.859135\n",
      "INFO:tensorflow:loss = 4.14390417705, step = 1101 (116.399 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.7793384686.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1c74dbd588>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.train(inputFunc, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-13-16:49:15\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpywtxm1q6/model.ckpt-1200\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-13-16:49:19\n",
      "INFO:tensorflow:Saving dict for global step 1200: accuracy = 0.681373, global_step = 1200, loss = 4.08465, ndcg = 0.908805, top1Classification = 0.506536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.68137252,\n",
       " 'global_step': 1200,\n",
       " 'loss': 4.08465,\n",
       " 'ndcg': 0.90880531,\n",
       " 'top1Classification': 0.50653595}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid, \"length\": lengthsValid}, y=outputsValidUnnorm.astype(float),\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(validFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    array1 = tf.constant([4, 3, 2, 1, 0])\n",
    "    print(tf.cast(tf.equal(array1[0], 4), tf.float64).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputsTrainUnnorm[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[0 1 2 4 3]': 1,\n",
       "         '[0 2 1 4 3]': 12,\n",
       "         '[0 2 4 1 3]': 83,\n",
       "         '[0 2 4 3 1]': 2,\n",
       "         '[1 2 4 0 3]': 1,\n",
       "         '[1 4 2 0 3]': 1,\n",
       "         '[1 4 2 3 0]': 1,\n",
       "         '[2 0 1 3 4]': 1,\n",
       "         '[2 0 1 4 3]': 37,\n",
       "         '[2 0 4 1 3]': 379,\n",
       "         '[2 0 4 3 1]': 5,\n",
       "         '[2 1 0 4 3]': 1,\n",
       "         '[2 1 4 0 3]': 5,\n",
       "         '[2 1 4 3 0]': 1,\n",
       "         '[2 3 1 4 0]': 1,\n",
       "         '[2 4 0 1 3]': 33,\n",
       "         '[2 4 0 3 1]': 3,\n",
       "         '[2 4 1 0 3]': 6,\n",
       "         '[2 4 1 3 0]': 1,\n",
       "         '[2 4 3 1 0]': 1,\n",
       "         '[3 1 4 2 0]': 1,\n",
       "         '[4 1 2 3 0]': 7,\n",
       "         '[4 1 3 0 2]': 38,\n",
       "         '[4 1 3 2 0]': 273,\n",
       "         '[4 2 0 1 3]': 3,\n",
       "         '[4 2 0 3 1]': 1,\n",
       "         '[4 2 1 0 3]': 4,\n",
       "         '[4 3 1 2 0]': 13})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(list(map(str, outputsTrainUnnorm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
