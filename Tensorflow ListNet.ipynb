{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "import solver\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(9, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData(rows, labels):\n",
    "    inputs = np.empty(shape=(rows, labels))\n",
    "    ordering = np.empty(shape=(rows, labels))\n",
    "    for i in range(rows):\n",
    "#         perm = np.random.permutation(np.arange(6, 6 + labels))\n",
    "        perm = np.random.permutation(labels)\n",
    "#         number = 0\n",
    "#         for j in range(labels):\n",
    "#             number += perm[j] * math.pow(10, labels - j - 1)\n",
    "            \n",
    "        ordering[i] = perm\n",
    "        inputs[i] = perm[::-1]\n",
    "\n",
    "#         inputs[i, 0] = number\n",
    "#         inputs[i, 1] = 0\n",
    "        \n",
    "    return (inputs, ordering)\n",
    "\n",
    "inputs, outputs = generateData(10000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = normalize(inputs)\n",
    "unnormalizedOutputs = outputs\n",
    "outputs = normalize(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = inputs.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "outputsTrain = outputs[0:trainSize]\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "outputsValid = outputs[trainSize:]\n",
    "unnormalizedOutputsValid = unnormalizedOutputs[trainSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36514837,  0.        ,  0.54772256,  0.73029674,  0.18257419],\n",
       "       [ 0.36514837,  0.73029674,  0.18257419,  0.        ,  0.54772256],\n",
       "       [ 0.36514837,  0.73029674,  0.18257419,  0.54772256,  0.        ],\n",
       "       ..., \n",
       "       [ 0.36514837,  0.73029674,  0.18257419,  0.54772256,  0.        ],\n",
       "       [ 0.        ,  0.73029674,  0.36514837,  0.54772256,  0.18257419],\n",
       "       [ 0.54772256,  0.        ,  0.18257419,  0.73029674,  0.36514837]])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2, 4, 3],\n",
       "       [3, 4, 0, 1, 2],\n",
       "       [2, 4, 1, 3, 0],\n",
       "       ..., \n",
       "       [4, 0, 2, 1, 3],\n",
       "       [0, 2, 4, 3, 1],\n",
       "       [3, 0, 4, 1, 2]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.argsort(outputsTrain)[::-1]\n",
    "# np.argsort(outputsTrain)[:,::-1]\n",
    "np.argsort(outputsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  3.  2.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    array1 = tf.constant([0, 1, 2, 3, 4], dtype=tf.float32)\n",
    "    print(tf.nn.top_k(array1, k=5).values.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 4 1]\n",
      "[  6.   9.   8.  10.   7.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    prediction = tf.constant([10, 6, 9, 8, 7], dtype=tf.float32)\n",
    "    actual = tf.constant([6, 7, 9, 8, 10], dtype=tf.float32)\n",
    "    argsort_indices = tf.nn.top_k(prediction, k=5).indices\n",
    "    print(argsort_indices.eval())\n",
    "    print(tf.gather(actual, argsort_indices).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 4, 3])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(outputsTrain[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.18257419,  0.36514837,  0.54772256,  0.73029674]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(np.array([0, 1, 2, 3, 4]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8257418583505536"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(normalize(np.array([0, 1, 2, 3, 4]).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    array1 = tf.constant([ 1,  3,  2,  0,  4], dtype=tf.float64)\n",
    "    array2 = tf.constant([ 1,  3,  2,  0,  4], dtype=tf.float64)\n",
    "    print(ndcg(array1, array2).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "#     print(outputsTrain[])\n",
    "#     print(tf.reduce_mean(ndcg(outputsTrain, outputsTrain)).eval())\n",
    "    c = tf.map_fn(lambda x: ndcg(x[0], x[1]), (outputsTrain, outputsTrain), dtype=tf.float64)\n",
    "    print(tf.reduce_mean(c).eval())\n",
    "#     print(tf.gather(outputsTrain, tf.nn.top_k(outputsTrain, k=5).indices).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = trainSize\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "STD = 0.1\n",
    "\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "inputFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsTrain}, y=outputsTrain,\n",
    "    batch_size=BATCH_SIZE, num_epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def network(xDict):\n",
    "    x = xDict[\"input\"]\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=ALPHA)\n",
    "    \n",
    "    # Hidden fully connected layer\n",
    "#     layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    \n",
    "    layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Hidden fully connected layer\n",
    "    layer2 = tf.layers.dense(flatten, 64, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    outLayer = tf.layers.dense(layer2, LABEL_COUNT)\n",
    "    return outLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler Divergence, as per https://stackoverflow.com/a/43298483\n",
    "def klDivergence(p, q):\n",
    "    pClipped = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "    qClipped = tf.clip_by_value(q, 1e-10, 1.0)\n",
    "    return tf.reduce_sum(pClipped * tf.log(pClipped/qClipped))\n",
    "\n",
    "# Loss function based off of Jensen-Shannon Divergence\n",
    "def loss(label, prediction):\n",
    "    mean = 0.5 * (label + prediction)\n",
    "    return 0.5 * klDivergence(label, mean) + 0.5 * klDivergence(prediction, mean)\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "# Accuracy metric using Normalized Discounted Cumulative Gain, as per https://github.com/shiba24/learning2rank/\n",
    "def ndcg(labels, predictions, k=5):\n",
    "    topK = tf.nn.top_k(labels, k=5)\n",
    "    sortedValues = topK.values\n",
    "    sortedIndices = topK.indices\n",
    "#         print(labelSorted)\n",
    "#         labelSorted = sorted(label, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(k):\n",
    "#             ideal_dcg += (2 ** labelSorted[:i] - 1.) / log2(tf.cast(i + 2, tf.float64))\n",
    "        ideal_dcg += (sortedValues[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    dcg = 0\n",
    "#         argsort_indices = np.argsort(predictions)[::-1]\n",
    "#         argsort_indices = tf.nn.top_k(predictions, k=5).indices\n",
    "#         print(argsort_indices)\n",
    "    for i in range(k):\n",
    "        dcg += (tf.gather(predictions, sortedIndices[i]) + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "#         dcg += (predictions[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    return dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def modelFunc(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    logits = network(features)\n",
    "    \n",
    "#     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "    # Predictions\n",
    "    # TODO: Possibly need to change\n",
    "    pred_classes = logits\n",
    "#     pred_classes = tf.argmax(logits, axis=1)\n",
    "#     pred_probas = tf.nn.softmax(logits)\n",
    "    pred_probas = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    print(logits.shape)\n",
    "#     print(resizedLogits.shape)\n",
    "    print(labels.shape)\n",
    "    print(pred_classes.shape)\n",
    "        \n",
    "    # Define loss and optimizer\n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "    loss_op = tf.reduce_mean(loss(labels, logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "#     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    ndcg_map = tf.map_fn(lambda x: ndcg(x[0], convertSinglePredToRank(x[1])), (labels, pred_classes), dtype=tf.float64)\n",
    "    acc_op = tf.metrics.mean(ndcg_map)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "#       train_op=train_op)\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmp7yg7a69i\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c22a98128>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.5\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmp7yg7a69i'}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "model = tf.estimator.Estimator(modelFunc, config=tf.contrib.learn.RunConfig(session_config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5)\n",
      "(?, 5)\n",
      "(?, 5)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmp7yg7a69i/model.ckpt.\n",
      "INFO:tensorflow:loss = 77.3557845153, step = 1\n",
      "INFO:tensorflow:global_step/sec: 757.805\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 101 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 1102.01\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 201 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1123.43\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 301 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1077.52\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 401 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1170.14\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 501 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1186.46\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 601 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1176.71\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 701 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1001.27\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 801 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1060.72\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 901 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1085.39\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1001 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1106.39\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1101 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1043.18\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1201 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 973.111\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1301 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 955.677\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1401 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.54\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1501 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1092.68\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1601 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1085.65\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1701 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1102.49\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1801 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1103.08\n",
      "INFO:tensorflow:loss = -26.5664104241, step = 1901 (0.091 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmp7yg7a69i/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: -26.5664104241.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1c22a98978>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.train(inputFunc, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5)\n",
      "(?, 5)\n",
      "(?, 5)\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-01-20:05:46\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmp7yg7a69i/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-01-20:05:47\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.85962, global_step = 2000, loss = -3.42646e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.85961992, 'global_step': 2000, 'loss': -3.4264641e-08}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid}, y=unnormalizedOutputsValid,\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(validFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid},\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "predictions = model.predict(validFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/2v/nktg94cn4cvfw3vprys2rgtm0000gn/T/tmpmnvjs6my/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(list(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.61369418,   9.71719172,  52.00292772,  10.6649038 ,\n",
       "         16.69486406],\n",
       "       [ 16.70020666,   9.89320897,  52.634171  ,  10.74836305,\n",
       "         16.89704599],\n",
       "       [ 16.65160719,   9.87297218,  52.27999647,  10.73323039,\n",
       "         16.82406887],\n",
       "       ..., \n",
       "       [ 16.97339922,  10.16850692,  53.58311961,  11.00491487,\n",
       "         17.19138965],\n",
       "       [ 16.92229189,  10.05589962,  53.27084152,  10.93379575,\n",
       "         17.14208514],\n",
       "       [ 16.43488798,   9.70844622,  51.66948251,  10.61542199,\n",
       "         16.59278829]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  0.,  2.,  1.,  4.],\n",
       "       [ 1.,  3.,  2.,  0.,  4.],\n",
       "       [ 4.,  0.,  1.,  3.,  2.],\n",
       "       ..., \n",
       "       [ 0.,  3.,  1.,  4.,  2.],\n",
       "       [ 2.,  1.,  0.,  4.,  3.],\n",
       "       [ 3.,  2.,  4.,  0.,  1.]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnormalizedOutputsValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0457548582\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "#     print(outputsTrain[])\n",
    "#     print(tf.reduce_mean(ndcg(outputsTrain, outputsTrain)).eval())\n",
    "#     c = tf.map_fn(lambda x: ndcg(x[0], x[1]), (outputsValid, predictions), dtype=tf.float64)\n",
    "#     print(tf.reduce_mean(c).eval())\n",
    "    print(ndcg(outputsValid[0], predictions[0]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertPredToRank(predictions):\n",
    "    array = np.empty(shape=predictions.shape)\n",
    "    for i, row in enumerate(predictions):\n",
    "        array[i] = np.argsort(predictions[i])[::-1].reshape(1, -1).astype(float)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertSinglePredToRank(prediction):\n",
    "    return tf.cast(tf.nn.top_k(prediction, k=5).indices, dtype=tf.float64)\n",
    "#     return np.argsort(prediction)[::-1].reshape(1, -1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  0.,  3.,  1.],\n",
       "       [ 2.,  4.,  0.,  3.,  1.],\n",
       "       [ 2.,  4.,  0.,  3.,  1.],\n",
       "       ..., \n",
       "       [ 2.,  4.,  0.,  3.,  1.],\n",
       "       [ 2.,  4.,  0.,  3.,  1.],\n",
       "       [ 2.,  4.,  0.,  3.,  1.]])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertPredToRank(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862375412267\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    c = tf.map_fn(lambda x: ndcg(x[0], x[1]), (unnormalizedOutputsValid, convertPredToRank(predictions)), dtype=tf.float64)\n",
    "    print(tf.reduce_mean(c).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826922872058\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    print(ndcg(unnormalizedOutputsValid[1], convertPredToRank(predictions)[1]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  1.,  0.,  4.]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(predictions[0])[::-1].reshape(1, -1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  4.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    array1 = tf.constant([0, 4, 1, 3, 2], dtype=tf.float64)\n",
    "    print(convertSinglePredToRank(array1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
