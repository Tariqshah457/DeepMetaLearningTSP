{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json\n",
    "import solver\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData(rows, labels):\n",
    "    inputs = np.empty(shape=(rows, labels))\n",
    "    ordering = np.empty(shape=(rows, labels))\n",
    "    for i in range(rows):\n",
    "#         perm = np.random.permutation(np.arange(6, 6 + labels))\n",
    "        perm = np.random.permutation(labels)\n",
    "#         number = 0\n",
    "#         for j in range(labels):\n",
    "#             number += perm[j] * math.pow(10, labels - j - 1)\n",
    "            \n",
    "        ordering[i] = perm\n",
    "        inputs[i] = perm[::-1]\n",
    "\n",
    "#         inputs[i, 0] = number\n",
    "#         inputs[i, 1] = 0\n",
    "        \n",
    "    return (inputs, ordering)\n",
    "\n",
    "inputs, outputs = generateData(10000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = normalize(inputs)\n",
    "unnormalizedOutputs = outputs\n",
    "outputs = normalize(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = inputs.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "outputsTrain = outputs[0:trainSize]\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "outputsValid = outputs[trainSize:]\n",
    "unnormalizedOutputsValid = unnormalizedOutputs[trainSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session().as_default():\n",
    "    array1 = tf.constant([ 1,  3,  2,  0,  4], dtype=tf.float64)\n",
    "    array2 = tf.constant([ 3,  2,  0,  4,  1], dtype=tf.float64)\n",
    "    print((spearmanCorrelation(array1, array2).eval() + 1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session().as_default():\n",
    "#     print(outputsTrain[])\n",
    "#     print(tf.reduce_mean(ndcg(outputsTrain, outputsTrain)).eval())\n",
    "    c = tf.map_fn(lambda x: boundedSpearman(x[0], x[1]), (outputsTrain, outputsTrain), dtype=tf.float64)\n",
    "    print(tf.reduce_mean(c).eval())\n",
    "#     print(tf.gather(outputsTrain, tf.nn.top_k(outputsTrain, k=5).indices).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = trainSize\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "STD = 0.1\n",
    "\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the input function for training\n",
    "inputFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsTrain}, y=outputsTrain,\n",
    "    batch_size=BATCH_SIZE, num_epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def network(xDict):\n",
    "    x = xDict[\"input\"]\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=ALPHA)\n",
    "    \n",
    "    # Hidden fully connected layer\n",
    "#     layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    \n",
    "    layer1 = tf.layers.dense(flatten, NODES1, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Hidden fully connected layer\n",
    "    layer2 = tf.layers.dense(flatten, 64, kernel_regularizer=regularizer, activation=tf.nn.relu)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    outLayer = tf.layers.dense(layer2, LABEL_COUNT)\n",
    "    return outLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kullback-Leibler Divergence, as per https://stackoverflow.com/a/43298483\n",
    "def klDivergence(p, q):\n",
    "    pClipped = tf.clip_by_value(p, 1e-10, 1.0)\n",
    "    qClipped = tf.clip_by_value(q, 1e-10, 1.0)\n",
    "    return tf.reduce_sum(pClipped * tf.log(pClipped/qClipped))\n",
    "\n",
    "# Loss function based off of Jensen-Shannon Divergence\n",
    "def loss(label, prediction):\n",
    "    mean = 0.5 * (label + prediction)\n",
    "    return 0.5 * klDivergence(label, mean) + 0.5 * klDivergence(prediction, mean)\n",
    "\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "# Accuracy metric using Normalized Discounted Cumulative Gain, as per https://github.com/shiba24/learning2rank/\n",
    "def ndcg(labels, predictions, k=5):\n",
    "    topK = tf.nn.top_k(labels, k=5)\n",
    "    sortedValues = topK.values\n",
    "    sortedIndices = topK.indices\n",
    "#         print(labelSorted)\n",
    "#         labelSorted = sorted(label, reverse=True)\n",
    "    ideal_dcg = 0\n",
    "    for i in range(k):\n",
    "#             ideal_dcg += (2 ** labelSorted[:i] - 1.) / log2(tf.cast(i + 2, tf.float64))\n",
    "        ideal_dcg += (sortedValues[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    dcg = 0\n",
    "#         argsort_indices = np.argsort(predictions)[::-1]\n",
    "#         argsort_indices = tf.nn.top_k(predictions, k=5).indices\n",
    "#         print(argsort_indices)\n",
    "    for i in range(k):\n",
    "        dcg += (tf.gather(predictions, sortedIndices[i]) + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "#         dcg += (predictions[i] + 1) / log2(tf.cast(i + 2, tf.float64))\n",
    "    return dcg / ideal_dcg\n",
    "\n",
    "def spearmanCorrelation(label, prediction):\n",
    "    length = tf.cast(tf.shape(prediction)[0], tf.float64)\n",
    "    sumVal = tf.reduce_sum(tf.square(tf.subtract(prediction, label)))\n",
    "    return 1 - 6 * sumVal / (length ** 3 - length)\n",
    "\n",
    "# Bound Spearman coeff. between 0 and 1\n",
    "def boundedSpearman(label, prediction):\n",
    "    return (spearmanCorrelation(label, prediction) + 1.)/2\n",
    "\n",
    "# Builds an integer ranking out of a 1-D tensor\n",
    "def convertPredToRank(prediction):\n",
    "    return tf.cast(tf.nn.top_k(prediction, k=5).indices, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def modelFunc(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    logits = network(features)\n",
    "    \n",
    "#     resizedLogits = tf.reshape(logits, shape=[-1, MAX_SIZE * MAX_SIZE, 1])\n",
    "    \n",
    "    # Predictions\n",
    "    # TODO: Possibly need to change\n",
    "#     pred_classes = logits\n",
    "    pred_classes = tf.map_fn(convertPredToRank, logits)\n",
    "#     pred_classes = tf.argmax(logits, axis=1)\n",
    "#     pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "#     loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#         logits=logits, labels=labels))\n",
    "    loss_op = tf.reduce_mean(loss(labels, logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "#     acc_op = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), predictions=pred_classes)\n",
    "#     acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    ndcg_map = tf.map_fn(lambda x: boundedSpearman(x[0], x[1]), (labels, pred_classes), dtype=tf.float64)\n",
    "    acc_op = tf.metrics.mean(ndcg_map)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Estimator\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "model = tf.estimator.Estimator(modelFunc, config=tf.contrib.learn.RunConfig(session_config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "model.train(inputFunc, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "validFunc = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input\": inputsValid}, y=unnormalizedOutputsValid,\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(validFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
