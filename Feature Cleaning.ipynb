{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "import pickle\n",
    "import solver\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTSPInstances(path):\n",
    "    dataframe = None\n",
    "    frameCreated = False\n",
    "    for path in glob.glob(path + \"*.json\"):\n",
    "        try:\n",
    "            with open(path) as file:\n",
    "                jsonDf = json.load(file)\n",
    "            newFrame = pd.io.json.json_normalize(jsonDf)\n",
    "            newFrame[\"name\"] = os.path.splitext(os.path.basename(path))[0]\n",
    "                        \n",
    "            if not frameCreated:\n",
    "                dataframe = newFrame\n",
    "                frameCreated = True\n",
    "            else:\n",
    "                dataframe = pd.concat([dataframe, newFrame])\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    cols = dataframe.columns.tolist()\n",
    "    cols.remove(\"name\")\n",
    "    cols.insert(0, \"name\")\n",
    "    dataframe = dataframe[cols]\n",
    "    \n",
    "    return dataframe.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genInstances = loadTSPInstances(\"../data/features/SymHeur15/generated/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genInstances[\"generated\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genInstances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = loadTSPInstances(\"../data/features/SymHeur15/tsplib/\")\n",
    "instances[\"generated\"] = False\n",
    "\n",
    "instances = genInstances.append(instances)\n",
    "instances = instances.reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "# Remove unnecessary tours\n",
    "instances = instances.drop([\"heuristics.simulatedAnnealingValues\", \"heuristics.graspValues\", \"heuristics.tabuValues\", \"heuristics.antColonyValues\", \"heuristics.geneticValues\"], axis=1)\n",
    "# Remove unimplemented features\n",
    "# instances = instances.drop([\"complexFeatures.entropyDegreeDistribution\", \"complexFeatures.vertexParticipationCoefficient\"], axis=1)\n",
    "# Replace all -1 values with NaN\n",
    "instances = instances.replace(-1, np.NaN)\n",
    "\n",
    "# Remove due to bug in creation\n",
    "# instances = instances.loc[instances[\"name\"] != \"pr2392\"]\n",
    "\n",
    "# Randomize instances\n",
    "instances = instances.reindex(np.random.permutation(instances.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTSPInstances(path, extension):\n",
    "    instances = []\n",
    "    for file in glob.glob(path + \"*.\" + extension):\n",
    "        try:\n",
    "            tsp = solver.loadTSPLib(file)\n",
    "            name = os.path.basename(file)\n",
    "            if not tsp:\n",
    "                print(\"Invalid file at \" + name)\n",
    "                continue\n",
    "\n",
    "            tsp.setName(name)\n",
    "            instances.append(tsp)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return instances\n",
    "\n",
    "def loadGeneratedInstances(path):\n",
    "    instances = []\n",
    "    for file in glob.glob(path + \"*.pytsp\"):\n",
    "        try:\n",
    "            tspFile = open(file, \"rb\")\n",
    "            tsp = pickle.load(tspFile)\n",
    "            name = os.path.basename(file)\n",
    "            \n",
    "            if not tsp:\n",
    "                print(\"Invalid file at \" + name)\n",
    "                continue\n",
    "\n",
    "            tsp.setName(name)\n",
    "            instances.append(tsp)\n",
    "            tspFile.close()\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "tspLibPath = \"../data/tsplib/tsp/\"\n",
    "atspLibPath = \"../data/tsplib/atsp/\"\n",
    "generatedPath = \"../data/generated2/\"\n",
    "\n",
    "tspLibInstances = loadTSPInstances(tspLibPath, \"tsp\")\n",
    "atspLibInstances = loadTSPInstances(atspLibPath, \"atsp\")\n",
    "generatedInstances = loadGeneratedInstances(generatedPath)\n",
    "\n",
    "allTSPInstances = tspLibInstances + atspLibInstances + generatedInstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge cost matrices into instances\n",
    "costInstances = pd.DataFrame(columns=[\"name\", \"costs\"])\n",
    "for instance in allTSPInstances:\n",
    "    name = os.path.splitext(os.path.basename(instance.getName()))[0]\n",
    "    costInstances = costInstances.append(pd.DataFrame([[name, instance.costs]], columns=[\"name\", \"costs\"]))\n",
    "costInstances = costInstances.reset_index().drop(\"index\", axis=1)\n",
    "instances = pd.merge(instances, costInstances, on=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "columnNames = list(instances)\n",
    "regexTimes = re.compile(\".*Times\")\n",
    "timesColumnNames = list(filter(regexTimes.match, columnNames))\n",
    "\n",
    "regexValues = re.compile(\".*Values\")\n",
    "valuesColumnNames = list(filter(regexValues.match, columnNames))\n",
    "\n",
    "regexCosts = re.compile(\"heuristics.*Costs\")\n",
    "heuristicCostsColumnNames = list(filter(regexCosts.match, columnNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/a/40449726\n",
    "def explode(df, lst_cols, fill_value=''):\n",
    "    # make sure `lst_cols` is a list\n",
    "    if lst_cols and not isinstance(lst_cols, list):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "\n",
    "    if (lens > 0).all():\n",
    "        # ALL lists in cells aren't empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .loc[:, df.columns]\n",
    "    else:\n",
    "        # at least one list in cells is empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n",
    "          .loc[:, df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deterministicColumnNames = list(columnNames)\n",
    "multivaluedColumnNames = timesColumnNames + valuesColumnNames + heuristicCostsColumnNames\n",
    "multivaluedColumnNames.remove(\"heldKarpTimes\")\n",
    "for column in multivaluedColumnNames:\n",
    "    try:\n",
    "        deterministicColumnNames.remove(column)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "instances = explode(instances, multivaluedColumnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert columns to numeric\n",
    "newInstances = pd.DataFrame()\n",
    "for column in list(instances):\n",
    "    if column != \"name\" and column != \"costs\":\n",
    "        numericColumn = instances[column].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    else:\n",
    "        numericColumn = instances[column]\n",
    "    newInstances = pd.concat([newInstances, numericColumn], axis=1)\n",
    "instances = newInstances\n",
    "# Replace all -1 values with NaN\n",
    "instances = instances.replace(-1, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group by name and compute means\n",
    "# group = instances.groupby([\"name\"])\n",
    "# averagedInstances = pd.DataFrame()\n",
    "# for column in list(instances):\n",
    "#     if column == \"name\":\n",
    "#         continue\n",
    "#     try:\n",
    "#         groupedMean = pd.DataFrame(group[column].mean())\n",
    "#         averagedInstances = pd.concat([averagedInstances, groupedMean], axis=1)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "size = instances.shape[0]\n",
    "trainValidSize = int(size * 0.8)\n",
    "testSize = size - trainValidSize\n",
    "\n",
    "averagedInstances = instances[0:trainValidSize]\n",
    "testInstances = instances[trainValidSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averagedInstances.to_pickle(\"../data/features/symHeur15analysis.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
