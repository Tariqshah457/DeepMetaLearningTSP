{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/features/analysis.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "columnNames = list(df)\n",
    "regexTimes = re.compile(\".*Times\")\n",
    "timesColumnNames = list(filter(regexTimes.match, columnNames))\n",
    "for column in timesColumnNames:\n",
    "    columnNames.remove(column)\n",
    "    \n",
    "regexCosts = re.compile(\"heuristics.*Costs\")\n",
    "costsColumnNames = list(filter(regexCosts.match, columnNames))\n",
    "for column in costsColumnNames:\n",
    "    columnNames.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 30 rows due to None values\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NA\n",
    "rowsBefore = df.shape[0]\n",
    "df = df.dropna()\n",
    "print(\"Dropped %d rows due to None values\" % (rowsBefore - df.shape[0]))\n",
    "\n",
    "minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)\n",
    "\n",
    "# Remove all *Times columns\n",
    "df = df[columnNames]\n",
    "\n",
    "# Remove name column\n",
    "df = df.drop([\"name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "intLabels = LabelEncoder().fit_transform(minCostIndices).reshape(-1, 1)\n",
    "# 5 values for 5 different heuristics\n",
    "outputs = OneHotEncoder(sparse=False, n_values=5).fit_transform(intLabels)\n",
    "\n",
    "inputs = scale(df.astype('float64'),axis=1)\n",
    "\n",
    "size = df.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "outputsTrain = outputs[0:trainSize]\n",
    "intLabelsTrain = intLabels[0:trainSize]\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "outputsValid = outputs[trainSize:]\n",
    "intLabelsValid = intLabels[trainSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsValid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_count = 0\n",
    "\n",
    "def minibatch(batchSize, n, input_data, output_data):\n",
    "    input_batches = np.empty((math.ceil(n/batchSize), batchSize) + input_data.shape[1:])\n",
    "    output_batches = np.empty((math.ceil(n/batchSize), batchSize) + output_data.shape[1:])\n",
    "    \n",
    "    global epoch_count\n",
    "    epoch_count += 1\n",
    "    indexes = np.random.permutation(n)\n",
    "    i = 0\n",
    "    batch_i = 0\n",
    "    input_array = np.zeros((batchSize,) + input_data.shape[1:])\n",
    "    output_array = np.zeros((batchSize,) + output_data.shape[1:])\n",
    "    for index in indexes:\n",
    "        input_array[i] = input_data[index]\n",
    "        output_array[i] = output_data[index]\n",
    "        i += 1\n",
    "\n",
    "        if i >= batchSize:\n",
    "            input_batches[batch_i] = input_array\n",
    "            output_batches[batch_i] = output_array\n",
    "            i = 0\n",
    "            batch_i += 1\n",
    "    \n",
    "    if(n % batchSize != 0):\n",
    "        input_array[i:] = input_data[0:batchSize - i]\n",
    "        output_array[i:] = output_data[0:batchSize - i]\n",
    "        input_batches[batch_i] = input_array\n",
    "        output_batches[batch_i] = output_array\n",
    "    \n",
    "    return (input_batches, output_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = trainSize\n",
    "FEATURE_COUNT = df.shape[1]\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "STD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup Tensorflow\n",
    "\n",
    "# Constants\n",
    "x_train_full = tf.constant(inputsTrain, dtype='float32', shape=[trainSize, FEATURE_COUNT])\n",
    "y_train_full = tf.constant(outputsTrain, dtype='float32', shape=[trainSize, LABEL_COUNT])\n",
    "\n",
    "x_valid_full = tf.constant(inputsValid, dtype='float32', shape=[validSize, FEATURE_COUNT])\n",
    "y_valid_full = tf.constant(outputsValid, dtype='float32', shape=[validSize, LABEL_COUNT])\n",
    "\n",
    "x_train = tf.placeholder(tf.float32, [BATCH_SIZE, FEATURE_COUNT])\n",
    "y_train = tf.placeholder(tf.float32, [BATCH_SIZE, LABEL_COUNT])\n",
    "\n",
    "# Variables\n",
    "W_input = tf.Variable(tf.truncated_normal([FEATURE_COUNT, NODES1], stddev=STD, seed = 0))\n",
    "b_input = tf.Variable(tf.truncated_normal([1, NODES1], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden = tf.Variable(tf.truncated_normal([NODES1, NODES2], stddev=STD, seed = 0))\n",
    "b_hidden = tf.Variable(tf.truncated_normal([1, NODES2], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden2 = tf.Variable(tf.truncated_normal([NODES2, LABEL_COUNT], stddev=STD, seed = 0))\n",
    "b_hidden2 = tf.Variable(tf.truncated_normal([1, LABEL_COUNT], stddev=STD, seed = 0))\n",
    "\n",
    "# Optimization\n",
    "input_layer = tf.nn.relu(tf.matmul(x_train, W_input) + b_input)\n",
    "\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W_hidden) + b_hidden)\n",
    "hidden2_layer = tf.matmul(hidden_layer, W_hidden2) + b_hidden2\n",
    "\n",
    "logits_train_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_train_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "logits_valid_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_valid_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "\n",
    "L2 = tf.reduce_mean(ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hidden2_layer, labels = y_train) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE_train_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_train_full, labels = y_train_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "CE_valid_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_valid_full, labels = y_valid_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "y_pred_train = tf.nn.softmax(logits_train_full)\n",
    "y_pred_valid = tf.nn.softmax(logits_valid_full)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TensorFlow\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printStats():\n",
    "    (ce_train,ce_valid,p_train,p_valid,l2) = sess.run([CE_train_full, CE_valid_full, y_pred_train, y_pred_valid, L2])\n",
    "    labels_train_pred = oneHotArray[p_train.argmax(axis=1)]\n",
    "    labels_valid_pred = oneHotArray[p_valid.argmax(axis=1)]\n",
    "    error_train = 1 - accuracy_score(intLabelsTrain, labels_train_pred)\n",
    "    error_valid = 1 - accuracy_score(intLabelsValid, labels_valid_pred)\n",
    "    total_compute_time = (time.time() - t_start)/60\n",
    "    print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' % (EPOCHS,epoch_count,ce_train,ce_valid,error_train,error_valid,l2,total_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          cross-entropy              error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "  10000       1     0.72422     1.30976       0.239       0.427    0.092604         0.0\n",
      "  10000     301     0.67823     1.13223       0.239       0.427    0.046218         0.1\n",
      "  10000     604     0.67314     1.16352       0.239       0.427    0.047923         0.2\n",
      "  10000     905     0.67354     1.15514       0.239       0.427    0.047492         0.3\n",
      "  10000    1203     0.67218     1.14527       0.239       0.427    0.045163         0.4\n",
      "  10000    1493     0.67375     1.11937       0.239       0.427    0.044583         0.5\n",
      "  10000    1776     0.67132     1.17175       0.239       0.427    0.045861         0.6\n",
      "  10000    2056     0.67228     1.16408       0.239       0.427    0.046610         0.7\n",
      "  10000    2325     0.67369     1.11384       0.239       0.427    0.043823         0.8\n",
      "  10000    2606     0.67228     1.14187       0.239       0.427    0.045343         0.9\n",
      "  10000    2873     0.67354     1.11565       0.239       0.427    0.042675         1.0\n",
      "  10000    3153     0.67199     1.13676       0.239       0.427    0.044105         1.1\n",
      "  10000    3436     0.67184     1.14951       0.239       0.427    0.045162         1.2\n",
      "  10000    3709     0.67196     1.14559       0.239       0.427    0.045343         1.2\n",
      "  10000    3983     0.67148     1.15130       0.239       0.427    0.044910         1.3\n",
      "  10000    4265     0.67146     1.16277       0.239       0.427    0.045632         1.4\n",
      "  10000    4544     0.67176     1.14307       0.239       0.427    0.044355         1.5\n",
      "  10000    4828     0.67116     1.16728       0.239       0.427    0.044914         1.6\n",
      "  10000    5112     0.67190     1.14324       0.239       0.427    0.044954         1.7\n",
      "  10000    5396     0.67128     1.17018       0.239       0.427    0.045415         1.8\n",
      "  10000    5681     0.67202     1.13596       0.239       0.427    0.043811         1.9\n",
      "  10000    5968     0.67179     1.13913       0.239       0.427    0.044151         2.0\n",
      "  10000    6252     0.67143     1.15179       0.239       0.427    0.044812         2.1\n",
      "  10000    6534     0.67151     1.14984       0.239       0.427    0.044704         2.2\n",
      "  10000    6823     0.67144     1.15564       0.239       0.427    0.044968         2.3\n",
      "  10000    7108     0.67159     1.14923       0.239       0.427    0.044844         2.4\n",
      "  10000    7393     0.67157     1.14801       0.239       0.427    0.044681         2.5\n",
      "  10000    7677     0.67153     1.15039       0.239       0.427    0.044703         2.6\n",
      "  10000    7962     0.67173     1.15516       0.239       0.427    0.045454         2.7\n",
      "  10000    8248     0.67171     1.14329       0.239       0.427    0.044322         2.8\n",
      "  10000    8532     0.67267     1.12600       0.239       0.427    0.043543         2.9\n",
      "  10000    8817     0.67234     1.13334       0.239       0.427    0.044198         3.0\n",
      "  10000    9101     0.67159     1.15403       0.239       0.427    0.045119         3.1\n",
      "  10000    9386     0.67152     1.15328       0.239       0.427    0.044893         3.2\n",
      "  10000    9659     0.67164     1.14432       0.239       0.427    0.044087         3.3\n",
      "  10000    9942     0.67184     1.14315       0.239       0.427    0.044593         3.4\n",
      "Finished\n",
      "Elapsed Time: 3.375089\n",
      "Epoch Count: 10000\n",
      "  10000   10000     0.67194     1.13832       0.239       0.427    0.044154         3.4\n"
     ]
    }
   ],
   "source": [
    "# Minimize MSE\n",
    "\n",
    "train = True\n",
    "\n",
    "oneHotArray = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "total_elapsed_time = 0\n",
    "\n",
    "ce_time = 0\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "print('%15s%24s%24s' % (' ','cross-entropy','error-rate'))\n",
    "print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "\n",
    "while(train):\n",
    "    batch = minibatch(BATCH_SIZE, N1, inputsTrain, outputsTrain)\n",
    "\n",
    "    for step in range(batch[0].shape[0]):\n",
    "        x_batch = batch[0][step]\n",
    "        y_batch = batch[1][step]\n",
    "        \n",
    "        t_start = time.time()\n",
    "        sess.run([optimizer], feed_dict={x_train:x_batch,y_train:y_batch})\n",
    "        t_end = time.time()\n",
    "        \n",
    "        total_elapsed_time += (t_end - t_start)/60\n",
    "        \n",
    "        if t_end - ce_time > 6:\n",
    "#             (ce) = sess.run(CE_train_full)\n",
    "#             print(\"cross-entropy = %f\" % (ce))\n",
    "            printStats()\n",
    "            \n",
    "            ce_time = time.time()\n",
    "\n",
    "            \n",
    "        if epoch_count >= EPOCHS:\n",
    "            train = False\n",
    "            break\n",
    "\n",
    "print(\"Finished\")\n",
    "print(\"Elapsed Time: %f\" % (total_elapsed_time))\n",
    "print(\"Epoch Count: %d\" % (epoch_count))\n",
    "\n",
    "printStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
