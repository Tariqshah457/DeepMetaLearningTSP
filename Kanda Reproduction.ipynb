{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/features/analysis.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "columnNames = list(df)\n",
    "regexTimes = re.compile(\".*Times\")\n",
    "timesColumnNames = list(filter(regexTimes.match, columnNames))\n",
    "for column in timesColumnNames:\n",
    "    columnNames.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all *Times columns\n",
    "df = df[columnNames]\n",
    "# Drop rows with NA\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)\n",
    "\n",
    "intLabels = LabelEncoder().fit_transform(minCostIndices).reshape(-1, 1)\n",
    "# 5 values for 5 different heuristics\n",
    "outputs = OneHotEncoder(sparse=False, n_values=5).fit_transform(intLabels)\n",
    "\n",
    "inputs = scale(df.astype('float64'),axis=1)\n",
    "\n",
    "inputsTrain = inputs[0:50]\n",
    "outputsTrain = outputs[0:50]\n",
    "intLabelsTrain = intLabels[0:50]\n",
    "\n",
    "inputsValid = inputs[50:]\n",
    "outputsValid = outputs[50:]\n",
    "intLabelsValid = intLabels[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 0\n",
    "\n",
    "def minibatch(batchSize, n, input_data, output_data):\n",
    "    input_batches = np.empty((math.ceil(n/batchSize), batchSize) + input_data.shape[1:])\n",
    "    output_batches = np.empty((math.ceil(n/batchSize), batchSize) + output_data.shape[1:])\n",
    "    \n",
    "    global epoch_count\n",
    "    epoch_count += 1\n",
    "    indexes = np.random.permutation(n)\n",
    "    i = 0\n",
    "    batch_i = 0\n",
    "    input_array = np.zeros((batchSize,) + input_data.shape[1:])\n",
    "    output_array = np.zeros((batchSize,) + output_data.shape[1:])\n",
    "    for index in indexes:\n",
    "        input_array[i] = input_data[index]\n",
    "        output_array[i] = output_data[index]\n",
    "        i += 1\n",
    "\n",
    "        if i >= batchSize:\n",
    "            input_batches[batch_i] = input_array\n",
    "            output_batches[batch_i] = output_array\n",
    "            i = 0\n",
    "            batch_i += 1\n",
    "    \n",
    "    if(n % batchSize != 0):\n",
    "        input_batches[batch_i] = input_array[0:i]\n",
    "        output_batches[batch_i] = output_array[0:i]\n",
    "    \n",
    "    return (input_batches, output_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = 50\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "STD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tensorflow\n",
    "\n",
    "# Constants\n",
    "x_train_full = tf.constant(inputsTrain, dtype='float32', shape=[50, 43])\n",
    "y_train_full = tf.constant(outputsTrain, dtype='float32', shape=[50, 5])\n",
    "\n",
    "x_valid_full = tf.constant(inputsValid, dtype='float32', shape=[17, 43])\n",
    "y_valid_full = tf.constant(outputsValid, dtype='float32', shape=[17, 5])\n",
    "\n",
    "x_train = tf.placeholder(tf.float32, [BATCH_SIZE, 43])\n",
    "y_train = tf.placeholder(tf.float32, [BATCH_SIZE, 5])\n",
    "\n",
    "# Variables\n",
    "W_input = tf.Variable(tf.truncated_normal([43, NODES1], stddev=STD, seed = 0))\n",
    "b_input = tf.Variable(tf.truncated_normal([1, NODES1], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden = tf.Variable(tf.truncated_normal([NODES1, NODES2], stddev=STD, seed = 0))\n",
    "b_hidden = tf.Variable(tf.truncated_normal([1, NODES2], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden2 = tf.Variable(tf.truncated_normal([NODES2, 5], stddev=STD, seed = 0))\n",
    "b_hidden2 = tf.Variable(tf.truncated_normal([1, 5], stddev=STD, seed = 0))\n",
    "\n",
    "# Optimization\n",
    "input_layer = tf.nn.relu(tf.matmul(x_train, W_input) + b_input)\n",
    "\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W_hidden) + b_hidden)\n",
    "hidden2_layer = tf.matmul(hidden_layer, W_hidden2) + b_hidden2\n",
    "\n",
    "logits_train_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_train_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "logits_valid_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_valid_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "\n",
    "L2 = tf.reduce_mean(ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hidden2_layer, labels = y_train) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE_train_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_train_full, labels = y_train_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "CE_valid_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_valid_full, labels = y_valid_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "y_pred_train = tf.nn.softmax(logits_train_full)\n",
    "y_pred_valid = tf.nn.softmax(logits_valid_full)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TensorFlow\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printStats():\n",
    "    (ce_train,ce_valid,p_train,p_valid,l2) = sess.run([CE_train_full, CE_valid_full, y_pred_train, y_pred_valid, L2])\n",
    "    labels_train_pred = oneHotArray[p_train.argmax(axis=1)]\n",
    "    labels_valid_pred = oneHotArray[p_valid.argmax(axis=1)]\n",
    "    error_train = 1 - accuracy_score(intLabelsTrain, labels_train_pred)\n",
    "    error_valid = 1 - accuracy_score(intLabelsValid, labels_valid_pred)\n",
    "    total_compute_time = (time.time() - t_start)/60\n",
    "    print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' % (EPOCHS,epoch_count,ce_train,ce_valid,error_train,error_valid,l2,total_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          cross-entropy              error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "  10000       1    48.25170    47.76961       0.280       0.059   47.115326         0.0\n",
      "  10000    3301     0.78259     0.62940       0.280       0.059    0.064813         0.1\n",
      "  10000    6393     0.78059     0.62734       0.280       0.059    0.063658         0.2\n",
      "  10000    9625     0.77991     0.62738       0.280       0.059    0.063190         0.3\n",
      "Finished\n",
      "Elapsed Time: 0.290357\n",
      "Epoch Count: 10000\n",
      "  10000   10000     0.77987     0.62923       0.280       0.059    0.063094         0.3\n"
     ]
    }
   ],
   "source": [
    "# Minimize MSE\n",
    "\n",
    "train = True\n",
    "\n",
    "oneHotArray = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "total_elapsed_time = 0\n",
    "\n",
    "ce_time = 0\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "print('%15s%24s%24s' % (' ','cross-entropy','error-rate'))\n",
    "print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "\n",
    "while(train):\n",
    "    batch = minibatch(BATCH_SIZE, N1, inputsTrain, outputsTrain)\n",
    "\n",
    "    for step in range(batch[0].shape[0]):\n",
    "        x_batch = batch[0][step]\n",
    "        y_batch = batch[1][step]\n",
    "        \n",
    "        t_start = time.time()\n",
    "        sess.run([optimizer], feed_dict={x_train:x_batch,y_train:y_batch})\n",
    "        t_end = time.time()\n",
    "        \n",
    "        total_elapsed_time += (t_end - t_start)/60\n",
    "        \n",
    "        if t_end - ce_time > 6:\n",
    "#             (ce) = sess.run(CE_train_full)\n",
    "#             print(\"cross-entropy = %f\" % (ce))\n",
    "            printStats()\n",
    "            \n",
    "            ce_time = time.time()\n",
    "\n",
    "            \n",
    "        if epoch_count >= EPOCHS:\n",
    "            train = False\n",
    "            break\n",
    "\n",
    "print(\"Finished\")\n",
    "print(\"Elapsed Time: %f\" % (total_elapsed_time))\n",
    "print(\"Epoch Count: %d\" % (epoch_count))\n",
    "\n",
    "printStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
