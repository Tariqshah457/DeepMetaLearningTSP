{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/features/analysis.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "columnNames = list(df)\n",
    "regexTimes = re.compile(\".*Times\")\n",
    "timesColumnNames = list(filter(regexTimes.match, columnNames))\n",
    "for column in timesColumnNames:\n",
    "    columnNames.remove(column)\n",
    "    \n",
    "regexCosts = re.compile(\"heuristics.*Costs\")\n",
    "costsColumnNames = list(filter(regexCosts.match, columnNames))\n",
    "for column in costsColumnNames:\n",
    "    columnNames.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NA\n",
    "df = df.dropna()\n",
    "\n",
    "minCostIndices = df[[\"heuristics.tabuCosts\", \"heuristics.simulatedAnnealingCosts\", \"heuristics.graspCosts\", \"heuristics.geneticCosts\", \"heuristics.antColonyCosts\"]].idxmin(axis=1)\n",
    "\n",
    "# Remove all *Times columns\n",
    "df = df[columnNames]\n",
    "\n",
    "# Remove name column\n",
    "df = df.drop([\"name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intLabels = LabelEncoder().fit_transform(minCostIndices).reshape(-1, 1)\n",
    "# 5 values for 5 different heuristics\n",
    "outputs = OneHotEncoder(sparse=False, n_values=5).fit_transform(intLabels)\n",
    "\n",
    "inputs = scale(df.astype('float64'),axis=1)\n",
    "\n",
    "size = df.shape[0]\n",
    "# Test data is separated in cleaning stage\n",
    "trainSize = int(size * 0.75)\n",
    "validSize = size - trainSize\n",
    "\n",
    "inputsTrain = inputs[0:trainSize]\n",
    "outputsTrain = outputs[0:trainSize]\n",
    "intLabelsTrain = intLabels[0:trainSize]\n",
    "\n",
    "inputsValid = inputs[trainSize:]\n",
    "outputsValid = outputs[trainSize:]\n",
    "intLabelsValid = intLabels[trainSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 38)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputsValid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_count = 0\n",
    "\n",
    "def minibatch(batchSize, n, input_data, output_data):\n",
    "    input_batches = np.empty((math.ceil(n/batchSize), batchSize) + input_data.shape[1:])\n",
    "    output_batches = np.empty((math.ceil(n/batchSize), batchSize) + output_data.shape[1:])\n",
    "    \n",
    "    global epoch_count\n",
    "    epoch_count += 1\n",
    "    indexes = np.random.permutation(n)\n",
    "    i = 0\n",
    "    batch_i = 0\n",
    "    input_array = np.zeros((batchSize,) + input_data.shape[1:])\n",
    "    output_array = np.zeros((batchSize,) + output_data.shape[1:])\n",
    "    for index in indexes:\n",
    "        input_array[i] = input_data[index]\n",
    "        output_array[i] = output_data[index]\n",
    "        i += 1\n",
    "\n",
    "        if i >= batchSize:\n",
    "            input_batches[batch_i] = input_array\n",
    "            output_batches[batch_i] = output_array\n",
    "            i = 0\n",
    "            batch_i += 1\n",
    "    \n",
    "    if(n % batchSize != 0):\n",
    "        input_batches[batch_i] = input_array[0:i]\n",
    "        output_batches[batch_i] = output_array[0:i]\n",
    "    \n",
    "    return (input_batches, output_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "N1 = trainSize\n",
    "FEATURE_COUNT = df.shape[1]\n",
    "LABEL_COUNT = 5\n",
    "\n",
    "NODES1 = 512\n",
    "NODES2 = 256\n",
    "\n",
    "ALPHA = 0.08\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "STD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tensorflow\n",
    "\n",
    "# Constants\n",
    "x_train_full = tf.constant(inputsTrain, dtype='float32', shape=[trainSize, FEATURE_COUNT])\n",
    "y_train_full = tf.constant(outputsTrain, dtype='float32', shape=[trainSize, LABEL_COUNT])\n",
    "\n",
    "x_valid_full = tf.constant(inputsValid, dtype='float32', shape=[validSize, FEATURE_COUNT])\n",
    "y_valid_full = tf.constant(outputsValid, dtype='float32', shape=[validSize, LABEL_COUNT])\n",
    "\n",
    "x_train = tf.placeholder(tf.float32, [BATCH_SIZE, FEATURE_COUNT])\n",
    "y_train = tf.placeholder(tf.float32, [BATCH_SIZE, LABEL_COUNT])\n",
    "\n",
    "# Variables\n",
    "W_input = tf.Variable(tf.truncated_normal([FEATURE_COUNT, NODES1], stddev=STD, seed = 0))\n",
    "b_input = tf.Variable(tf.truncated_normal([1, NODES1], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden = tf.Variable(tf.truncated_normal([NODES1, NODES2], stddev=STD, seed = 0))\n",
    "b_hidden = tf.Variable(tf.truncated_normal([1, NODES2], stddev=STD, seed = 0))\n",
    "\n",
    "W_hidden2 = tf.Variable(tf.truncated_normal([NODES2, LABEL_COUNT], stddev=STD, seed = 0))\n",
    "b_hidden2 = tf.Variable(tf.truncated_normal([1, LABEL_COUNT], stddev=STD, seed = 0))\n",
    "\n",
    "# Optimization\n",
    "input_layer = tf.nn.relu(tf.matmul(x_train, W_input) + b_input)\n",
    "\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W_hidden) + b_hidden)\n",
    "hidden2_layer = tf.matmul(hidden_layer, W_hidden2) + b_hidden2\n",
    "\n",
    "logits_train_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_train_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "logits_valid_full = tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(x_valid_full, W_input) + b_input), W_hidden) + b_hidden), W_hidden2) + b_hidden2\n",
    "\n",
    "L2 = tf.reduce_mean(ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hidden2_layer, labels = y_train) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "CE_train_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_train_full, labels = y_train_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "CE_valid_full = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_valid_full, labels = y_valid_full) + ALPHA * (tf.nn.l2_loss(W_input) + tf.nn.l2_loss(W_hidden) + tf.nn.l2_loss(W_hidden2)))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(CE)\n",
    "\n",
    "y_pred_train = tf.nn.softmax(logits_train_full)\n",
    "y_pred_valid = tf.nn.softmax(logits_valid_full)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TensorFlow\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printStats():\n",
    "    (ce_train,ce_valid,p_train,p_valid,l2) = sess.run([CE_train_full, CE_valid_full, y_pred_train, y_pred_valid, L2])\n",
    "    labels_train_pred = oneHotArray[p_train.argmax(axis=1)]\n",
    "    labels_valid_pred = oneHotArray[p_valid.argmax(axis=1)]\n",
    "    error_train = 1 - accuracy_score(intLabelsTrain, labels_train_pred)\n",
    "    error_valid = 1 - accuracy_score(intLabelsValid, labels_valid_pred)\n",
    "    total_compute_time = (time.time() - t_start)/60\n",
    "    print('%7d %7d%12.5f%12.5f%12.3f%12.3f%12f%12.1f' % (EPOCHS,epoch_count,ce_train,ce_valid,error_train,error_valid,l2,total_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          cross-entropy              error-rate\n",
      "          epoch    training  validation    training  validation          L2  time (min)\n",
      "  10000       1    47.27028    48.05124       0.184       0.478   46.361668         0.0\n",
      "  10000     753     0.63446     1.39320       0.184       0.478    0.062967         0.1\n",
      "  10000    1445     0.62784     1.40938       0.184       0.478    0.061407         0.2\n",
      "  10000    2111     0.62774     1.27604       0.184       0.478    0.060334         0.3\n",
      "  10000    2758     0.62726     1.41323       0.184       0.478    0.055474         0.4\n",
      "  10000    3390     0.62334     1.29253       0.184       0.478    0.057848         0.5\n",
      "  10000    4024     0.62801     1.22141       0.184       0.478    0.054002         0.6\n",
      "  10000    4637     0.62358     1.32045       0.184       0.478    0.058533         0.7\n",
      "  10000    5264     0.63782     1.17536       0.184       0.478    0.047802         0.8\n",
      "  10000    5869     0.62120     1.34739       0.184       0.478    0.054918         0.9\n",
      "  10000    6469     0.62307     1.32385       0.184       0.478    0.055243         1.0\n",
      "  10000    7060     0.62638     1.34603       0.184       0.478    0.064319         1.1\n",
      "  10000    7657     0.62144     1.39924       0.184       0.478    0.056504         1.1\n",
      "  10000    8235     0.62065     1.29544       0.184       0.478    0.051737         1.2\n",
      "  10000    8807     0.62135     1.41193       0.184       0.478    0.058334         1.3\n",
      "  10000    9382     0.62047     1.34870       0.184       0.478    0.053697         1.4\n",
      "  10000    9946     0.62251     1.35345       0.184       0.478    0.056724         1.5\n",
      "Finished\n",
      "Elapsed Time: 1.539476\n",
      "Epoch Count: 10000\n",
      "  10000   10000     0.61963     1.33948       0.184       0.478    0.057197         1.5\n"
     ]
    }
   ],
   "source": [
    "# Minimize MSE\n",
    "\n",
    "train = True\n",
    "\n",
    "oneHotArray = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "total_elapsed_time = 0\n",
    "\n",
    "ce_time = 0\n",
    "\n",
    "epoch_count = 0\n",
    "\n",
    "print('%15s%24s%24s' % (' ','cross-entropy','error-rate'))\n",
    "print('%15s%12s%12s%12s%12s%12s%12s' % ('epoch','training','validation','training','validation','L2','time (min)'))\n",
    "\n",
    "while(train):\n",
    "    batch = minibatch(BATCH_SIZE, N1, inputsTrain, outputsTrain)\n",
    "\n",
    "    for step in range(batch[0].shape[0]):\n",
    "        x_batch = batch[0][step]\n",
    "        y_batch = batch[1][step]\n",
    "        \n",
    "        t_start = time.time()\n",
    "        sess.run([optimizer], feed_dict={x_train:x_batch,y_train:y_batch})\n",
    "        t_end = time.time()\n",
    "        \n",
    "        total_elapsed_time += (t_end - t_start)/60\n",
    "        \n",
    "        if t_end - ce_time > 6:\n",
    "#             (ce) = sess.run(CE_train_full)\n",
    "#             print(\"cross-entropy = %f\" % (ce))\n",
    "            printStats()\n",
    "            \n",
    "            ce_time = time.time()\n",
    "\n",
    "            \n",
    "        if epoch_count >= EPOCHS:\n",
    "            train = False\n",
    "            break\n",
    "\n",
    "print(\"Finished\")\n",
    "print(\"Elapsed Time: %f\" % (total_elapsed_time))\n",
    "print(\"Epoch Count: %d\" % (epoch_count))\n",
    "\n",
    "printStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
